{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "effecientnet_disaster_Adam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RV05/Classification-of-disasters-using-modified-architrctures/blob/main/effecientnet_disaster_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01lAEMug9uHN",
        "outputId": "8660138d-cedf-43a0-98d0-90325bae0111"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsYuPMHtVa8p",
        "outputId": "eb60c67a-70ed-4548-ee14-b2f18542d6d7"
      },
      "source": [
        "cd \"/content/drive/MyDrive/SCAAI_Rohit_volety_Problem_Statement_3/deep learning\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SCAAI_Rohit_volety_Problem_Statement_3/deep learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT5oQ9GQegzo"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "import config\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "# \thelp=\"path to output loss/accuracy plot\")\n",
        "# args = vars(ap.parse_args())\n",
        "# # determine the total number of image paths in training, validation,\n",
        "# # and testing directories\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2W2oRSbfwYi"
      },
      "source": [
        "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
        "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
        "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
        "trainAug = ImageDataGenerator(\n",
        "\t# rotation_range=25,\n",
        "\t# zoom_range=0.1,\n",
        "\t# width_shift_range=0.1,\n",
        "\t# height_shift_range=0.1,\n",
        "\t# shear_range=0.2,\n",
        "\t# horizontal_flip=True,\n",
        "  # vertical_flip=True,\n",
        "\t# fill_mode=\"nearest\"\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWJ8ZBoSf8hg"
      },
      "source": [
        "# initialize the validation/testing data augmentation object (which\n",
        "# we'll be adding mean subtraction to)\n",
        "valAug = ImageDataGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lIkzyUDgAfo"
      },
      "source": [
        "# define the ImageNet mean subtraction (in RGB order) and set the\n",
        "# the mean subtraction value for each of the data augmentation\n",
        "# objects\n",
        "# mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "# trainAug.mean = mean\n",
        "# valAug.mean = mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph4f5GXagD-7",
        "outputId": "d0e63fba-8b3d-4f68-bb90-b7b93b0a4bde"
      },
      "source": [
        "trainGen = trainAug.flow_from_directory(\n",
        "\tconfig.TRAIN_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=config.BS)\n",
        "# initialize the validation generator\n",
        "valGen = valAug.flow_from_directory(\n",
        "\tconfig.VAL_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)\n",
        "# initialize the testing generator\n",
        "testGen = valAug.flow_from_directory(\n",
        "\tconfig.TEST_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3309 images belonging to 6 classes.\n",
            "Found 31 images belonging to 6 classes.\n",
            "Found 1022 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4po5oj4TggC_",
        "outputId": "bfa57f74-718a-4bae-b6f8-ad5ac0b0d68c"
      },
      "source": [
        "print(\"[INFO] preparing model...\")\n",
        "baseModel = EfficientNetB7(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(200, 200, 3)))\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D()(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(len(config.CLASSES), activation=\"softmax\")(headModel)\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "# compile the model\n",
        "opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] preparing model...\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 1s 0us/step\n",
            "258088960/258076736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7cBdl5Of3aG",
        "outputId": "a891d80e-aa6d-4bfc-edea-5acdfb367023"
      },
      "source": [
        "\n",
        "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit_generator(\n",
        "\ttrainGen,\n",
        "\tsteps_per_epoch=totalTrain // config.BS,\n",
        "\tvalidation_data=valGen,\n",
        "\tvalidation_steps=totalVal // config.BS,\n",
        "\tepochs=config.NUM_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "413/413 [==============================] - 895s 2s/step - loss: 0.8266 - accuracy: 0.7089 - val_loss: 0.7231 - val_accuracy: 0.8333\n",
            "Epoch 2/200\n",
            "413/413 [==============================] - 42s 102ms/step - loss: 0.3968 - accuracy: 0.8667 - val_loss: 1.1071 - val_accuracy: 0.7083\n",
            "Epoch 3/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.2523 - accuracy: 0.9143 - val_loss: 1.1450 - val_accuracy: 0.7500\n",
            "Epoch 4/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.1857 - accuracy: 0.9349 - val_loss: 0.9283 - val_accuracy: 0.7500\n",
            "Epoch 5/200\n",
            "413/413 [==============================] - 44s 105ms/step - loss: 0.1367 - accuracy: 0.9549 - val_loss: 1.1473 - val_accuracy: 0.7083\n",
            "Epoch 6/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0965 - accuracy: 0.9636 - val_loss: 1.1047 - val_accuracy: 0.7917\n",
            "Epoch 7/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0994 - accuracy: 0.9697 - val_loss: 1.2012 - val_accuracy: 0.7500\n",
            "Epoch 8/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0854 - accuracy: 0.9676 - val_loss: 1.5206 - val_accuracy: 0.7500\n",
            "Epoch 9/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0704 - accuracy: 0.9761 - val_loss: 1.1632 - val_accuracy: 0.7917\n",
            "Epoch 10/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0780 - accuracy: 0.9733 - val_loss: 1.0073 - val_accuracy: 0.7917\n",
            "Epoch 11/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0623 - accuracy: 0.9764 - val_loss: 1.2415 - val_accuracy: 0.7917\n",
            "Epoch 12/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0608 - accuracy: 0.9806 - val_loss: 0.7583 - val_accuracy: 0.8750\n",
            "Epoch 13/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0553 - accuracy: 0.9809 - val_loss: 0.6652 - val_accuracy: 0.8333\n",
            "Epoch 14/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0477 - accuracy: 0.9818 - val_loss: 1.1016 - val_accuracy: 0.7083\n",
            "Epoch 15/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0376 - accuracy: 0.9873 - val_loss: 0.9387 - val_accuracy: 0.7917\n",
            "Epoch 16/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0464 - accuracy: 0.9818 - val_loss: 1.0283 - val_accuracy: 0.8333\n",
            "Epoch 17/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0507 - accuracy: 0.9821 - val_loss: 2.0007 - val_accuracy: 0.7500\n",
            "Epoch 18/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0348 - accuracy: 0.9858 - val_loss: 1.4454 - val_accuracy: 0.7917\n",
            "Epoch 19/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0563 - accuracy: 0.9800 - val_loss: 0.9858 - val_accuracy: 0.8333\n",
            "Epoch 20/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0360 - accuracy: 0.9861 - val_loss: 1.5555 - val_accuracy: 0.7500\n",
            "Epoch 21/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0407 - accuracy: 0.9870 - val_loss: 1.1729 - val_accuracy: 0.7917\n",
            "Epoch 22/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0285 - accuracy: 0.9876 - val_loss: 2.0925 - val_accuracy: 0.7500\n",
            "Epoch 23/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0542 - accuracy: 0.9830 - val_loss: 1.5565 - val_accuracy: 0.7500\n",
            "Epoch 24/200\n",
            "413/413 [==============================] - 44s 105ms/step - loss: 0.0240 - accuracy: 0.9912 - val_loss: 1.4220 - val_accuracy: 0.7500\n",
            "Epoch 25/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0217 - accuracy: 0.9906 - val_loss: 1.5958 - val_accuracy: 0.6667\n",
            "Epoch 26/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0401 - accuracy: 0.9855 - val_loss: 1.9894 - val_accuracy: 0.7083\n",
            "Epoch 27/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0581 - accuracy: 0.9839 - val_loss: 1.2896 - val_accuracy: 0.7083\n",
            "Epoch 28/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0363 - accuracy: 0.9846 - val_loss: 1.3693 - val_accuracy: 0.7917\n",
            "Epoch 29/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0234 - accuracy: 0.9888 - val_loss: 2.0092 - val_accuracy: 0.6667\n",
            "Epoch 30/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0295 - accuracy: 0.9882 - val_loss: 1.5323 - val_accuracy: 0.7500\n",
            "Epoch 31/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0322 - accuracy: 0.9882 - val_loss: 1.7036 - val_accuracy: 0.6667\n",
            "Epoch 32/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0235 - accuracy: 0.9888 - val_loss: 1.4958 - val_accuracy: 0.7083\n",
            "Epoch 33/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0399 - accuracy: 0.9873 - val_loss: 1.7643 - val_accuracy: 0.7083\n",
            "Epoch 34/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0208 - accuracy: 0.9900 - val_loss: 1.6385 - val_accuracy: 0.7500\n",
            "Epoch 35/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0465 - accuracy: 0.9846 - val_loss: 1.7656 - val_accuracy: 0.6667\n",
            "Epoch 36/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0252 - accuracy: 0.9894 - val_loss: 1.2885 - val_accuracy: 0.7500\n",
            "Epoch 37/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0147 - accuracy: 0.9924 - val_loss: 1.5102 - val_accuracy: 0.7083\n",
            "Epoch 38/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0277 - accuracy: 0.9882 - val_loss: 1.9538 - val_accuracy: 0.7083\n",
            "Epoch 39/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0151 - accuracy: 0.9930 - val_loss: 2.3334 - val_accuracy: 0.6667\n",
            "Epoch 40/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0087 - accuracy: 0.9945 - val_loss: 2.0025 - val_accuracy: 0.6667\n",
            "Epoch 41/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0417 - accuracy: 0.9858 - val_loss: 1.3299 - val_accuracy: 0.7500\n",
            "Epoch 42/200\n",
            "413/413 [==============================] - 44s 105ms/step - loss: 0.0319 - accuracy: 0.9870 - val_loss: 1.7457 - val_accuracy: 0.7083\n",
            "Epoch 43/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0158 - accuracy: 0.9918 - val_loss: 1.2982 - val_accuracy: 0.7917\n",
            "Epoch 44/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: 1.3056 - val_accuracy: 0.7500\n",
            "Epoch 45/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0258 - accuracy: 0.9879 - val_loss: 2.0282 - val_accuracy: 0.7500\n",
            "Epoch 46/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 1.6734 - val_accuracy: 0.7917\n",
            "Epoch 47/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0302 - accuracy: 0.9867 - val_loss: 1.7059 - val_accuracy: 0.7083\n",
            "Epoch 48/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0180 - accuracy: 0.9906 - val_loss: 1.5267 - val_accuracy: 0.7083\n",
            "Epoch 49/200\n",
            "413/413 [==============================] - 44s 105ms/step - loss: 0.0125 - accuracy: 0.9942 - val_loss: 1.6565 - val_accuracy: 0.7500\n",
            "Epoch 50/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 1.3800 - val_accuracy: 0.7500\n",
            "Epoch 51/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 1.7523 - val_accuracy: 0.7083\n",
            "Epoch 52/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0247 - accuracy: 0.9873 - val_loss: 1.5443 - val_accuracy: 0.7083\n",
            "Epoch 53/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0166 - accuracy: 0.9924 - val_loss: 1.2513 - val_accuracy: 0.7917\n",
            "Epoch 54/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0110 - accuracy: 0.9945 - val_loss: 1.3343 - val_accuracy: 0.7500\n",
            "Epoch 55/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0155 - accuracy: 0.9912 - val_loss: 1.5263 - val_accuracy: 0.7500\n",
            "Epoch 56/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0227 - accuracy: 0.9897 - val_loss: 1.8044 - val_accuracy: 0.7917\n",
            "Epoch 57/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0252 - accuracy: 0.9885 - val_loss: 1.9285 - val_accuracy: 0.7500\n",
            "Epoch 58/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0270 - accuracy: 0.9879 - val_loss: 1.8371 - val_accuracy: 0.7083\n",
            "Epoch 59/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0241 - accuracy: 0.9909 - val_loss: 1.7516 - val_accuracy: 0.7083\n",
            "Epoch 60/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0266 - accuracy: 0.9888 - val_loss: 2.7905 - val_accuracy: 0.6667\n",
            "Epoch 61/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0158 - accuracy: 0.9912 - val_loss: 1.7087 - val_accuracy: 0.7500\n",
            "Epoch 62/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0163 - accuracy: 0.9912 - val_loss: 1.9161 - val_accuracy: 0.7083\n",
            "Epoch 63/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0209 - accuracy: 0.9885 - val_loss: 1.8648 - val_accuracy: 0.7917\n",
            "Epoch 64/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0163 - accuracy: 0.9933 - val_loss: 2.2431 - val_accuracy: 0.7083\n",
            "Epoch 65/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0194 - accuracy: 0.9906 - val_loss: 2.2254 - val_accuracy: 0.7083\n",
            "Epoch 66/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0381 - accuracy: 0.9858 - val_loss: 1.8666 - val_accuracy: 0.7500\n",
            "Epoch 67/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0200 - accuracy: 0.9903 - val_loss: 2.1823 - val_accuracy: 0.7083\n",
            "Epoch 68/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0201 - accuracy: 0.9915 - val_loss: 2.0497 - val_accuracy: 0.7500\n",
            "Epoch 69/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0157 - accuracy: 0.9912 - val_loss: 1.7113 - val_accuracy: 0.7500\n",
            "Epoch 70/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0206 - accuracy: 0.9906 - val_loss: 1.2977 - val_accuracy: 0.8750\n",
            "Epoch 71/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0212 - accuracy: 0.9900 - val_loss: 2.2697 - val_accuracy: 0.7083\n",
            "Epoch 72/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0169 - accuracy: 0.9930 - val_loss: 1.8291 - val_accuracy: 0.7917\n",
            "Epoch 73/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0109 - accuracy: 0.9933 - val_loss: 1.7117 - val_accuracy: 0.8333\n",
            "Epoch 74/200\n",
            "413/413 [==============================] - 44s 105ms/step - loss: 0.0128 - accuracy: 0.9921 - val_loss: 1.9937 - val_accuracy: 0.7500\n",
            "Epoch 75/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0145 - accuracy: 0.9924 - val_loss: 1.3324 - val_accuracy: 0.8750\n",
            "Epoch 76/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0310 - accuracy: 0.9876 - val_loss: 1.8709 - val_accuracy: 0.7083\n",
            "Epoch 77/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0108 - accuracy: 0.9918 - val_loss: 1.8702 - val_accuracy: 0.7500\n",
            "Epoch 78/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0091 - accuracy: 0.9945 - val_loss: 2.4518 - val_accuracy: 0.7500\n",
            "Epoch 79/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0267 - accuracy: 0.9912 - val_loss: 1.7037 - val_accuracy: 0.7500\n",
            "Epoch 80/200\n",
            "413/413 [==============================] - 44s 105ms/step - loss: 0.0276 - accuracy: 0.9888 - val_loss: 1.4085 - val_accuracy: 0.7500\n",
            "Epoch 81/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0098 - accuracy: 0.9933 - val_loss: 1.6984 - val_accuracy: 0.7917\n",
            "Epoch 82/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0101 - accuracy: 0.9936 - val_loss: 1.7935 - val_accuracy: 0.7500\n",
            "Epoch 83/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0209 - accuracy: 0.9909 - val_loss: 1.8324 - val_accuracy: 0.7500\n",
            "Epoch 84/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 2.6411 - val_accuracy: 0.7083\n",
            "Epoch 85/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0200 - accuracy: 0.9918 - val_loss: 1.7794 - val_accuracy: 0.7500\n",
            "Epoch 86/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0230 - accuracy: 0.9903 - val_loss: 1.3947 - val_accuracy: 0.7917\n",
            "Epoch 87/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0166 - accuracy: 0.9927 - val_loss: 1.5584 - val_accuracy: 0.8333\n",
            "Epoch 88/200\n",
            "413/413 [==============================] - 43s 104ms/step - loss: 0.0246 - accuracy: 0.9897 - val_loss: 1.3650 - val_accuracy: 0.8333\n",
            "Epoch 89/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0167 - accuracy: 0.9927 - val_loss: 1.4484 - val_accuracy: 0.8333\n",
            "Epoch 90/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0178 - accuracy: 0.9912 - val_loss: 1.9801 - val_accuracy: 0.7917\n",
            "Epoch 91/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0182 - accuracy: 0.9912 - val_loss: 1.1748 - val_accuracy: 0.7917\n",
            "Epoch 92/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 1.4942 - val_accuracy: 0.7500\n",
            "Epoch 93/200\n",
            "413/413 [==============================] - 43s 105ms/step - loss: 0.0163 - accuracy: 0.9924 - val_loss: 1.4400 - val_accuracy: 0.7917\n",
            "Epoch 94/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.5053 - val_accuracy: 0.8333\n",
            "Epoch 95/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 2.0828 - val_accuracy: 0.7917\n",
            "Epoch 96/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0132 - accuracy: 0.9924 - val_loss: 3.6313 - val_accuracy: 0.7083\n",
            "Epoch 97/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0343 - accuracy: 0.9882 - val_loss: 2.4283 - val_accuracy: 0.7083\n",
            "Epoch 98/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0114 - accuracy: 0.9936 - val_loss: 2.3202 - val_accuracy: 0.7083\n",
            "Epoch 99/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0107 - accuracy: 0.9942 - val_loss: 2.7968 - val_accuracy: 0.7917\n",
            "Epoch 100/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0149 - accuracy: 0.9927 - val_loss: 2.5892 - val_accuracy: 0.7917\n",
            "Epoch 101/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0191 - accuracy: 0.9909 - val_loss: 2.5631 - val_accuracy: 0.7083\n",
            "Epoch 102/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0134 - accuracy: 0.9915 - val_loss: 1.5019 - val_accuracy: 0.7500\n",
            "Epoch 103/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0365 - accuracy: 0.9876 - val_loss: 1.8259 - val_accuracy: 0.7917\n",
            "Epoch 104/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0188 - accuracy: 0.9915 - val_loss: 1.5166 - val_accuracy: 0.7917\n",
            "Epoch 105/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0170 - accuracy: 0.9921 - val_loss: 1.8791 - val_accuracy: 0.7917\n",
            "Epoch 106/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0154 - accuracy: 0.9927 - val_loss: 1.6849 - val_accuracy: 0.7917\n",
            "Epoch 107/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0152 - accuracy: 0.9921 - val_loss: 2.0108 - val_accuracy: 0.7917\n",
            "Epoch 108/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0078 - accuracy: 0.9952 - val_loss: 2.0402 - val_accuracy: 0.7917\n",
            "Epoch 109/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0082 - accuracy: 0.9936 - val_loss: 2.2303 - val_accuracy: 0.7500\n",
            "Epoch 110/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0082 - accuracy: 0.9936 - val_loss: 1.7056 - val_accuracy: 0.7917\n",
            "Epoch 111/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0083 - accuracy: 0.9942 - val_loss: 2.7062 - val_accuracy: 0.7500\n",
            "Epoch 112/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0082 - accuracy: 0.9942 - val_loss: 2.5358 - val_accuracy: 0.7917\n",
            "Epoch 113/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0085 - accuracy: 0.9927 - val_loss: 3.0329 - val_accuracy: 0.7500\n",
            "Epoch 114/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 2.5452 - val_accuracy: 0.7500\n",
            "Epoch 115/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0247 - accuracy: 0.9897 - val_loss: 2.4230 - val_accuracy: 0.7500\n",
            "Epoch 116/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0088 - accuracy: 0.9927 - val_loss: 2.4898 - val_accuracy: 0.7917\n",
            "Epoch 117/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0137 - accuracy: 0.9930 - val_loss: 2.0388 - val_accuracy: 0.8333\n",
            "Epoch 118/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0221 - accuracy: 0.9912 - val_loss: 1.8161 - val_accuracy: 0.8750\n",
            "Epoch 119/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0129 - accuracy: 0.9927 - val_loss: 2.5591 - val_accuracy: 0.7917\n",
            "Epoch 120/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0195 - accuracy: 0.9930 - val_loss: 2.5572 - val_accuracy: 0.7917\n",
            "Epoch 121/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0093 - accuracy: 0.9930 - val_loss: 2.4169 - val_accuracy: 0.7500\n",
            "Epoch 122/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0176 - accuracy: 0.9921 - val_loss: 1.8660 - val_accuracy: 0.7917\n",
            "Epoch 123/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0128 - accuracy: 0.9945 - val_loss: 2.1697 - val_accuracy: 0.7917\n",
            "Epoch 124/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0279 - accuracy: 0.9909 - val_loss: 1.7399 - val_accuracy: 0.7917\n",
            "Epoch 125/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0097 - accuracy: 0.9949 - val_loss: 1.8951 - val_accuracy: 0.7500\n",
            "Epoch 126/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 1.9948 - val_accuracy: 0.7917\n",
            "Epoch 127/200\n",
            "413/413 [==============================] - 46s 110ms/step - loss: 0.0150 - accuracy: 0.9927 - val_loss: 1.8306 - val_accuracy: 0.7917\n",
            "Epoch 128/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 1.9399 - val_accuracy: 0.7917\n",
            "Epoch 129/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0145 - accuracy: 0.9918 - val_loss: 2.2099 - val_accuracy: 0.7917\n",
            "Epoch 130/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 1.9768 - val_accuracy: 0.7917\n",
            "Epoch 131/200\n",
            "413/413 [==============================] - 46s 110ms/step - loss: 0.0157 - accuracy: 0.9918 - val_loss: 2.1353 - val_accuracy: 0.7917\n",
            "Epoch 132/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0089 - accuracy: 0.9930 - val_loss: 2.8797 - val_accuracy: 0.7500\n",
            "Epoch 133/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0103 - accuracy: 0.9939 - val_loss: 3.2127 - val_accuracy: 0.7500\n",
            "Epoch 134/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0214 - accuracy: 0.9921 - val_loss: 1.8381 - val_accuracy: 0.7500\n",
            "Epoch 135/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.3352 - val_accuracy: 0.7917\n",
            "Epoch 136/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0198 - accuracy: 0.9921 - val_loss: 4.1191 - val_accuracy: 0.7500\n",
            "Epoch 137/200\n",
            "413/413 [==============================] - 46s 111ms/step - loss: 0.0136 - accuracy: 0.9945 - val_loss: 2.4593 - val_accuracy: 0.7500\n",
            "Epoch 138/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0122 - accuracy: 0.9945 - val_loss: 2.5323 - val_accuracy: 0.7500\n",
            "Epoch 139/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0165 - accuracy: 0.9915 - val_loss: 3.5388 - val_accuracy: 0.7083\n",
            "Epoch 140/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0103 - accuracy: 0.9939 - val_loss: 3.5715 - val_accuracy: 0.7500\n",
            "Epoch 141/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0090 - accuracy: 0.9936 - val_loss: 3.1965 - val_accuracy: 0.7083\n",
            "Epoch 142/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0234 - accuracy: 0.9906 - val_loss: 2.3508 - val_accuracy: 0.7917\n",
            "Epoch 143/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0159 - accuracy: 0.9897 - val_loss: 3.6419 - val_accuracy: 0.7500\n",
            "Epoch 144/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0086 - accuracy: 0.9939 - val_loss: 3.8309 - val_accuracy: 0.7083\n",
            "Epoch 145/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0274 - accuracy: 0.9888 - val_loss: 2.1886 - val_accuracy: 0.7083\n",
            "Epoch 146/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0180 - accuracy: 0.9912 - val_loss: 3.0413 - val_accuracy: 0.7500\n",
            "Epoch 147/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0211 - accuracy: 0.9909 - val_loss: 2.7171 - val_accuracy: 0.7500\n",
            "Epoch 148/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0190 - accuracy: 0.9906 - val_loss: 1.7293 - val_accuracy: 0.7083\n",
            "Epoch 149/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0087 - accuracy: 0.9949 - val_loss: 2.1158 - val_accuracy: 0.7083\n",
            "Epoch 150/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0083 - accuracy: 0.9933 - val_loss: 2.5241 - val_accuracy: 0.7917\n",
            "Epoch 151/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0246 - accuracy: 0.9927 - val_loss: 2.7150 - val_accuracy: 0.7500\n",
            "Epoch 152/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0097 - accuracy: 0.9936 - val_loss: 3.3391 - val_accuracy: 0.7500\n",
            "Epoch 153/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0085 - accuracy: 0.9945 - val_loss: 2.4807 - val_accuracy: 0.7917\n",
            "Epoch 154/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0098 - accuracy: 0.9927 - val_loss: 2.1561 - val_accuracy: 0.8333\n",
            "Epoch 155/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0085 - accuracy: 0.9927 - val_loss: 2.4974 - val_accuracy: 0.7500\n",
            "Epoch 156/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0189 - accuracy: 0.9915 - val_loss: 2.3097 - val_accuracy: 0.7917\n",
            "Epoch 157/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0195 - accuracy: 0.9921 - val_loss: 2.3190 - val_accuracy: 0.7500\n",
            "Epoch 158/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0092 - accuracy: 0.9945 - val_loss: 2.3779 - val_accuracy: 0.7917\n",
            "Epoch 159/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0303 - accuracy: 0.9894 - val_loss: 2.6823 - val_accuracy: 0.7500\n",
            "Epoch 160/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0116 - accuracy: 0.9942 - val_loss: 1.8933 - val_accuracy: 0.7500\n",
            "Epoch 161/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0087 - accuracy: 0.9949 - val_loss: 2.1515 - val_accuracy: 0.7500\n",
            "Epoch 162/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0125 - accuracy: 0.9909 - val_loss: 2.2311 - val_accuracy: 0.7500\n",
            "Epoch 163/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0083 - accuracy: 0.9939 - val_loss: 2.6158 - val_accuracy: 0.7500\n",
            "Epoch 164/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0076 - accuracy: 0.9949 - val_loss: 2.6863 - val_accuracy: 0.7500\n",
            "Epoch 165/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0388 - accuracy: 0.9903 - val_loss: 2.1632 - val_accuracy: 0.7917\n",
            "Epoch 166/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0219 - accuracy: 0.9903 - val_loss: 2.5295 - val_accuracy: 0.7500\n",
            "Epoch 167/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0203 - accuracy: 0.9924 - val_loss: 1.7380 - val_accuracy: 0.8333\n",
            "Epoch 168/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0172 - accuracy: 0.9918 - val_loss: 2.2970 - val_accuracy: 0.7083\n",
            "Epoch 169/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 3.3677 - val_accuracy: 0.7917\n",
            "Epoch 170/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0127 - accuracy: 0.9924 - val_loss: 2.5861 - val_accuracy: 0.7917\n",
            "Epoch 171/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0234 - accuracy: 0.9933 - val_loss: 2.6767 - val_accuracy: 0.7917\n",
            "Epoch 172/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0084 - accuracy: 0.9949 - val_loss: 3.4827 - val_accuracy: 0.7500\n",
            "Epoch 173/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0089 - accuracy: 0.9952 - val_loss: 3.6384 - val_accuracy: 0.7917\n",
            "Epoch 174/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0201 - accuracy: 0.9921 - val_loss: 2.4310 - val_accuracy: 0.7917\n",
            "Epoch 175/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0148 - accuracy: 0.9921 - val_loss: 2.8487 - val_accuracy: 0.7917\n",
            "Epoch 176/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0124 - accuracy: 0.9921 - val_loss: 2.5797 - val_accuracy: 0.7500\n",
            "Epoch 177/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0136 - accuracy: 0.9933 - val_loss: 1.3779 - val_accuracy: 0.7917\n",
            "Epoch 178/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0102 - accuracy: 0.9924 - val_loss: 3.4224 - val_accuracy: 0.7500\n",
            "Epoch 179/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0084 - accuracy: 0.9930 - val_loss: 1.8251 - val_accuracy: 0.7500\n",
            "Epoch 180/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0173 - accuracy: 0.9936 - val_loss: 1.9765 - val_accuracy: 0.7917\n",
            "Epoch 181/200\n",
            "413/413 [==============================] - 45s 110ms/step - loss: 0.0136 - accuracy: 0.9942 - val_loss: 2.3827 - val_accuracy: 0.7500\n",
            "Epoch 182/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0097 - accuracy: 0.9921 - val_loss: 2.6516 - val_accuracy: 0.7917\n",
            "Epoch 183/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0077 - accuracy: 0.9945 - val_loss: 2.8966 - val_accuracy: 0.7917\n",
            "Epoch 184/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0389 - accuracy: 0.9912 - val_loss: 1.0847 - val_accuracy: 0.8333\n",
            "Epoch 185/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0107 - accuracy: 0.9936 - val_loss: 1.9129 - val_accuracy: 0.8333\n",
            "Epoch 186/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0105 - accuracy: 0.9924 - val_loss: 1.3636 - val_accuracy: 0.8333\n",
            "Epoch 187/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0141 - accuracy: 0.9930 - val_loss: 1.2116 - val_accuracy: 0.7917\n",
            "Epoch 188/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0098 - accuracy: 0.9924 - val_loss: 1.3285 - val_accuracy: 0.8333\n",
            "Epoch 189/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0171 - accuracy: 0.9921 - val_loss: 1.5746 - val_accuracy: 0.7917\n",
            "Epoch 190/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0118 - accuracy: 0.9924 - val_loss: 1.6314 - val_accuracy: 0.7917\n",
            "Epoch 191/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0118 - accuracy: 0.9936 - val_loss: 1.4118 - val_accuracy: 0.7500\n",
            "Epoch 192/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0147 - accuracy: 0.9921 - val_loss: 1.1613 - val_accuracy: 0.7500\n",
            "Epoch 193/200\n",
            "413/413 [==============================] - 45s 109ms/step - loss: 0.0217 - accuracy: 0.9936 - val_loss: 1.4548 - val_accuracy: 0.7500\n",
            "Epoch 194/200\n",
            "413/413 [==============================] - 45s 108ms/step - loss: 0.0163 - accuracy: 0.9918 - val_loss: 1.1146 - val_accuracy: 0.7500\n",
            "Epoch 195/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0121 - accuracy: 0.9933 - val_loss: 1.6390 - val_accuracy: 0.7083\n",
            "Epoch 196/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0117 - accuracy: 0.9945 - val_loss: 1.1513 - val_accuracy: 0.7500\n",
            "Epoch 197/200\n",
            "413/413 [==============================] - 44s 106ms/step - loss: 0.0188 - accuracy: 0.9906 - val_loss: 1.9520 - val_accuracy: 0.7083\n",
            "Epoch 198/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0127 - accuracy: 0.9918 - val_loss: 1.7760 - val_accuracy: 0.7083\n",
            "Epoch 199/200\n",
            "413/413 [==============================] - 44s 107ms/step - loss: 0.0101 - accuracy: 0.9921 - val_loss: 1.5932 - val_accuracy: 0.7500\n",
            "Epoch 200/200\n",
            "413/413 [==============================] - 44s 108ms/step - loss: 0.0085 - accuracy: 0.9930 - val_loss: 1.7138 - val_accuracy: 0.7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj41CXepgq1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194f9119-2de2-46d0-f173-27260c978460"
      },
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "testGen.reset()\n",
        "predIdxs = model.predict_generator(testGen,\n",
        "\tsteps=(totalTest // config.BS) + 1)\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testGen.classes, predIdxs,\n",
        "\ttarget_names=testGen.class_indices.keys()))\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
        "N = config.NUM_EPOCHS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fire       0.92      0.82      0.87       200\n",
            "       human       0.82      0.88      0.85        42\n",
            "       infra       0.46      0.82      0.59       210\n",
            "        land       0.68      0.60      0.64       148\n",
            "    nodamage       0.40      0.07      0.11       211\n",
            "       water       0.69      0.84      0.76       211\n",
            "\n",
            "    accuracy                           0.64      1022\n",
            "   macro avg       0.66      0.67      0.64      1022\n",
            "weighted avg       0.63      0.64      0.60      1022\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUGR_HTRgNhC"
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"effecientnet disaster_Adam_plot.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEcaVZFjhvEF"
      },
      "source": [
        "H.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CFJSWm9slzUh",
        "outputId": "43e95fcd-0bf0-473e-fb23-9cb78717115f"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "dir_path='/content/drive/MyDrive/SCAAI_Rohit_volety_Problem_Statement_3/deep learning/disaster/training/human'\n",
        "model=tf.keras.models.load_model(\"/content/drive/MyDrive/SCAAI_Rohit_volety_Problem_Statement_3/deep learning/disaster.model\")\n",
        "for i in os.listdir(dir_path):\n",
        "  img=image.load_img(dir_path+'//'+i,target_size=(200,200))\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "  X=image.img_to_array(img)\n",
        "  X=np.expand_dims(X,axis=0)\n",
        "  images=np.vstack([X])\n",
        "  val=model.predict(images)\n",
        "  print(val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.8719549e-37 1.0000000e+00 1.7749625e-24 8.4715123e-34 0.0000000e+00\n",
            "  1.6505122e-33]]\n",
            "[[2.1110801e-23 1.0000000e+00 5.1492648e-17 7.3332938e-21 2.1149735e-23\n",
            "  2.3988142e-20]]\n",
            "[[0.0000000e+00 1.0000000e+00 8.8340455e-35 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[8.0834020e-12 1.0000000e+00 1.7948553e-08 2.9935759e-10 3.0806564e-12\n",
            "  4.5450846e-10]]\n",
            "[[2.7012701e-28 1.0000000e+00 2.3428782e-21 1.1564394e-26 5.2462426e-31\n",
            "  1.9086257e-26]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[3.6842388e-07 9.9989498e-01 7.2184754e-05 3.1510470e-05 4.0660321e-08\n",
            "  9.4531634e-07]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.4161914e-34 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[1.2163773e-15 1.0000000e+00 3.0667124e-10 5.4054980e-14 7.8933750e-17\n",
            "  1.3279307e-13]]\n",
            "[[1.0556992e-22 1.0000000e+00 3.1145153e-16 1.8179249e-20 2.2051709e-24\n",
            "  1.4044352e-20]]\n",
            "[[3.3110684e-27 1.0000000e+00 2.4481816e-19 2.9832148e-24 5.9925467e-29\n",
            "  8.9149707e-24]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.1011451e-32 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[1.0072399e-22 1.0000000e+00 2.9844992e-14 6.1718143e-21 1.0239128e-24\n",
            "  3.1111248e-20]]\n",
            "[[1.1740169e-37 1.0000000e+00 5.0045637e-25 1.3729238e-33 0.0000000e+00\n",
            "  1.0325997e-34]]\n",
            "[[3.8664852e-20 1.0000000e+00 5.7562299e-14 2.3524656e-17 1.7262406e-19\n",
            "  1.3187608e-13]]\n",
            "[[1.1475562e-37 1.0000000e+00 1.7559614e-27 5.3850441e-35 0.0000000e+00\n",
            "  3.7352659e-35]]\n",
            "[[0.0000000e+00 1.0000000e+00 3.3659288e-36 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[2.4846539e-30 1.0000000e+00 4.7976900e-22 3.1138532e-29 3.1857309e-34\n",
            "  1.7356816e-28]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.5934791e-37 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[1.6295579e-21 1.0000000e+00 1.4542146e-15 2.4112193e-19 5.1741746e-24\n",
            "  1.2317089e-19]]\n",
            "[[3.0650295e-13 1.0000000e+00 3.6706140e-09 8.1106146e-12 2.9688122e-14\n",
            "  1.1878602e-11]]\n",
            "[[6.0015643e-20 1.0000000e+00 7.0107413e-14 2.2223896e-17 4.3212291e-21\n",
            "  1.5389169e-17]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[6.8327812e-19 1.0000000e+00 2.0199545e-12 9.6924756e-17 8.2667114e-21\n",
            "  1.9227823e-17]]\n",
            "[[2.1004523e-31 1.0000000e+00 1.0843318e-21 1.7710014e-29 1.7333474e-34\n",
            "  8.0726288e-28]]\n",
            "[[3.23679099e-32 1.00000000e+00 4.56532226e-24 1.09676595e-29\n",
            "  1.76557271e-35 2.31257975e-30]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.9116768e-38 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[2.5094060e-10 9.9996912e-01 4.9512391e-07 3.0377245e-05 2.6854505e-10\n",
            "  4.5753040e-08]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[4.9329760e-17 1.0000000e+00 4.3710213e-11 2.5692341e-14 1.8018401e-18\n",
            "  1.6594922e-15]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.1465995e-34 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[9.4100939e-23 1.0000000e+00 4.5926748e-17 2.0767670e-22 1.3966155e-25\n",
            "  2.6886654e-21]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.000000e+00 1.000000e+00 2.183813e-34 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.0677831e-33 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.2317368e-30 0.0000000e+00 0.0000000e+00\n",
            "  2.8639340e-38]]\n",
            "[[0.000000e+00 1.000000e+00 8.702188e-37 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[0.000000e+00 1.000000e+00 8.600822e-33 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.5912191e-30 2.4037860e-37 0.0000000e+00\n",
            "  1.3869230e-37]]\n",
            "[[3.0248824e-23 1.0000000e+00 2.5139038e-15 5.8105702e-21 6.6209946e-26\n",
            "  6.8928263e-22]]\n",
            "[[2.0040606e-37 1.0000000e+00 2.0660221e-28 5.6339029e-36 0.0000000e+00\n",
            "  2.6184808e-35]]\n",
            "[[0.0000000e+00 1.0000000e+00 4.8328814e-31 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 4.0674277e-31 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[2.26417986e-27 1.00000000e+00 1.00693795e-19 3.18136140e-25\n",
            "  8.41343173e-30 3.12142350e-25]]\n",
            "[[3.1544725e-33 1.0000000e+00 3.2388439e-23 1.6355289e-30 3.5590525e-35\n",
            "  2.5875371e-30]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[1.2488922e-20 1.0000000e+00 7.7647585e-14 2.2297253e-18 2.9893125e-22\n",
            "  7.4051574e-18]]\n",
            "[[7.7162774e-09 9.9981469e-01 1.3142006e-05 1.5622615e-07 1.4586507e-07\n",
            "  1.7181282e-04]]\n",
            "[[3.5486760e-38 1.0000000e+00 7.9310246e-29 2.6874274e-34 0.0000000e+00\n",
            "  4.9267626e-35]]\n",
            "[[0.0000000e+00 1.0000000e+00 3.0872074e-28 2.2881253e-36 0.0000000e+00\n",
            "  1.0830875e-36]]\n",
            "[[0.000000e+00 1.000000e+00 8.778224e-33 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[6.6426456e-12 9.9999988e-01 6.2172951e-08 1.4060866e-08 7.5333186e-13\n",
            "  6.9073941e-11]]\n",
            "[[9.48505132e-24 1.00000000e+00 1.07738654e-17 1.18110656e-20\n",
            "  4.01280576e-25 3.67895363e-21]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[4.9592711e-27 1.0000000e+00 6.6097410e-18 1.5092095e-24 1.9262566e-30\n",
            "  2.1969498e-25]]\n",
            "[[5.0203807e-31 1.0000000e+00 2.6520566e-20 5.2364163e-27 2.6503718e-32\n",
            "  8.1727723e-28]]\n",
            "[[8.5354372e-14 1.0000000e+00 8.7903267e-09 6.3311539e-10 4.3500309e-14\n",
            "  6.8942816e-12]]\n",
            "[[8.9485295e-32 1.0000000e+00 6.2938993e-22 3.0657341e-26 3.3567001e-34\n",
            "  3.2392541e-29]]\n",
            "[[3.2661770e-23 1.0000000e+00 1.5639454e-16 2.1778554e-20 5.2138363e-26\n",
            "  1.2415905e-21]]\n",
            "[[1.4330255e-28 1.0000000e+00 2.7730396e-21 2.1824230e-26 2.8961677e-31\n",
            "  2.3324656e-26]]\n",
            "[[2.7086498e-34 1.0000000e+00 1.2253936e-23 5.1504832e-32 4.4537542e-38\n",
            "  9.9417294e-31]]\n",
            "[[1.6719970e-23 1.0000000e+00 5.2431793e-17 7.8016405e-20 1.0858031e-25\n",
            "  4.2406246e-21]]\n",
            "[[0.0000000e+00 1.0000000e+00 8.8879524e-29 1.8256588e-36 0.0000000e+00\n",
            "  2.5553551e-36]]\n",
            "[[6.2657230e-31 1.0000000e+00 2.3781190e-23 6.4125553e-29 2.6104935e-34\n",
            "  3.4967302e-28]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[1.2687530e-20 1.0000000e+00 1.3958604e-12 1.7502657e-17 1.5982022e-23\n",
            "  8.6524829e-19]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[3.5547296e-32 1.0000000e+00 2.3222880e-21 5.8995501e-29 1.6743420e-35\n",
            "  3.1639273e-29]]\n",
            "[[1.0467606e-37 1.0000000e+00 1.5334650e-27 2.3998327e-33 3.1850926e-38\n",
            "  3.1144306e-32]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[1.9968081e-25 1.0000000e+00 4.9159000e-17 4.4419114e-23 8.1668510e-28\n",
            "  4.2968152e-23]]\n",
            "[[7.7617553e-17 1.0000000e+00 3.3843362e-12 5.2447820e-16 9.7342938e-20\n",
            "  5.1575047e-16]]\n",
            "[[2.3915760e-29 1.0000000e+00 1.9109846e-20 1.7948893e-27 1.1001878e-32\n",
            "  5.9556787e-27]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[3.2231062e-22 1.0000000e+00 7.4541712e-15 7.1464964e-21 3.1589649e-25\n",
            "  1.4543328e-20]]\n",
            "[[1.9892640e-12 1.0000000e+00 5.7474715e-08 1.2243223e-10 7.0598136e-14\n",
            "  3.0924086e-11]]\n",
            "[[0.000000e+00 1.000000e+00 1.234818e-35 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[7.7655628e-28 1.0000000e+00 2.0512040e-20 1.2830353e-24 4.0659251e-30\n",
            "  3.5710927e-25]]\n",
            "[[0.0000000e+00 1.0000000e+00 4.7869166e-32 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[6.9183240e-30 1.0000000e+00 2.5195027e-21 5.3016996e-27 1.6561002e-33\n",
            "  5.3776114e-28]]\n",
            "[[0.000000e+00 1.000000e+00 7.847988e-37 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.0000000e+00 1.0000000e+00 7.5390895e-34 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.0000000e+00 1.0000000e+00 3.8753595e-35 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[5.3620127e-17 1.0000000e+00 5.8272003e-12 9.4632596e-16 1.7290745e-19\n",
            "  3.8683222e-16]]\n",
            "[[1.1916395e-17 1.0000000e+00 4.6961142e-12 2.1347517e-15 4.5817162e-19\n",
            "  1.2474860e-15]]\n",
            "[[8.3673203e-20 1.0000000e+00 6.9446639e-14 3.9131126e-17 2.3470826e-21\n",
            "  3.7726016e-18]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.00590396e-29 1.79764856e-37\n",
            "  0.00000000e+00 9.29439793e-38]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.2913839e-30 1.3572366e-37 0.0000000e+00\n",
            "  5.0286240e-38]]\n",
            "[[3.2122614e-11 1.0000000e+00 4.9928395e-09 3.8105533e-11 2.8827824e-11\n",
            "  9.8605880e-11]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.5365717e-38 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[5.3790723e-34 1.0000000e+00 5.2512471e-24 1.6678886e-30 2.8144730e-37\n",
            "  1.0087387e-31]]\n",
            "[[3.9280593e-29 1.0000000e+00 3.4136177e-21 2.5337114e-27 6.8630576e-34\n",
            "  3.4371513e-28]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[3.2738137e-29 1.0000000e+00 7.7772999e-21 1.7503380e-26 4.0020910e-32\n",
            "  1.6630763e-26]]\n",
            "[[9.1227675e-38 1.0000000e+00 2.5584102e-27 1.1239726e-33 0.0000000e+00\n",
            "  7.4660518e-34]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.2430237e-33 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.00000000e+00 1.00000000e+00 6.05052607e-28 1.18567496e-35\n",
            "  0.00000000e+00 9.67749329e-36]]\n",
            "[[4.5554815e-29 1.0000000e+00 7.4000954e-20 1.2201908e-26 1.0720697e-31\n",
            "  1.8625286e-26]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[5.7873667e-10 9.9999678e-01 1.6052820e-06 1.4426796e-06 4.0834558e-10\n",
            "  1.1582305e-07]]\n",
            "[[9.5890147e-19 1.0000000e+00 9.3982981e-14 4.3279874e-16 9.1903346e-20\n",
            "  1.2456784e-16]]\n",
            "[[2.4922778e-29 1.0000000e+00 6.5480225e-20 3.3340856e-27 2.0546845e-32\n",
            "  4.0411750e-27]]\n",
            "[[4.3001173e-35 1.0000000e+00 1.7614185e-24 3.8813147e-31 1.5350081e-37\n",
            "  9.0825549e-31]]\n",
            "[[3.5217230e-28 1.0000000e+00 2.4810011e-20 2.1575976e-26 4.9117015e-32\n",
            "  1.2949383e-26]]\n",
            "[[1.2994203e-10 9.9999857e-01 1.4271478e-06 4.6864354e-08 3.3718656e-11\n",
            "  1.4365951e-09]]\n",
            "[[1.1992155e-22 1.0000000e+00 1.1613081e-16 1.9667555e-20 2.6438190e-24\n",
            "  1.9667629e-20]]\n",
            "[[4.1072149e-25 1.0000000e+00 2.5708368e-18 2.0729362e-22 3.7128636e-27\n",
            "  2.0315230e-23]]\n",
            "[[2.3117159e-09 9.9999726e-01 2.5794261e-06 1.0087286e-08 8.2062984e-10\n",
            "  6.2902785e-08]]\n",
            "[[3.2475265e-32 1.0000000e+00 7.7345650e-24 7.0244547e-31 6.5329709e-37\n",
            "  3.0298030e-30]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[6.3025358e-26 1.0000000e+00 1.5410434e-17 7.2206408e-23 1.2919032e-27\n",
            "  4.3096023e-23]]\n",
            "[[0.0000000e+00 1.0000000e+00 3.1098686e-34 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[1.3083437e-20 1.0000000e+00 2.6584040e-15 2.4145132e-20 1.9668126e-23\n",
            "  5.0142872e-19]]\n",
            "[[1.1145283e-33 1.0000000e+00 1.3757887e-22 7.7112646e-31 3.9593045e-37\n",
            "  2.3622789e-30]]\n",
            "[[5.49315954e-28 1.00000000e+00 1.56486359e-17 2.92619374e-25\n",
            "  1.41900045e-30 3.02064775e-25]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[4.0805622e-30 1.0000000e+00 3.0796067e-22 5.6650646e-29 6.2485783e-33\n",
            "  2.8427823e-25]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.3846421e-30 1.7550074e-38 0.0000000e+00\n",
            "  3.3361408e-38]]\n",
            "[[1.6886863e-33 1.0000000e+00 3.3454950e-23 3.9862316e-31 2.7302871e-36\n",
            "  1.3015677e-29]]\n",
            "[[9.9426501e-29 1.0000000e+00 3.0882519e-23 9.2427227e-27 3.0802342e-34\n",
            "  3.0238251e-28]]\n",
            "[[3.7601947e-37 1.0000000e+00 7.6042486e-27 5.6879384e-34 0.0000000e+00\n",
            "  2.2093290e-33]]\n",
            "[[1.13285115e-08 9.99981999e-01 1.74916549e-05 3.50530541e-07\n",
            "  6.23772989e-09 1.62278781e-07]]\n",
            "[[0.00000000e+00 1.00000000e+00 1.12823995e-30 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.3739524e-38 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.000000e+00 1.000000e+00 4.534755e-32 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[8.5158018e-12 1.0000000e+00 2.0645885e-09 1.5282100e-11 1.1496563e-12\n",
            "  4.1014019e-12]]\n",
            "[[0.0000000e+00 1.0000000e+00 3.0447594e-31 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 3.2904163e-36 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.3163907e-32 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 7.1718344e-30 1.2984337e-36 0.0000000e+00\n",
            "  7.3371752e-38]]\n",
            "[[6.9874745e-36 1.0000000e+00 2.4152046e-27 2.5966400e-33 0.0000000e+00\n",
            "  2.3579989e-33]]\n",
            "[[2.4608083e-30 1.0000000e+00 8.4869975e-21 1.7541193e-28 5.7229109e-35\n",
            "  2.1337525e-28]]\n",
            "[[4.1715462e-24 1.0000000e+00 6.2356725e-18 5.8332454e-21 1.6289518e-25\n",
            "  1.6812791e-21]]\n",
            "[[1.1920480e-18 1.0000000e+00 1.5367700e-13 1.5017778e-17 2.4433103e-20\n",
            "  2.3720198e-17]]\n",
            "[[7.3952243e-17 1.0000000e+00 3.5715794e-12 1.5582182e-14 2.5492947e-17\n",
            "  1.1848195e-14]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[1.3315591e-22 1.0000000e+00 1.2208959e-15 1.9018214e-20 1.4538419e-24\n",
            "  6.4093528e-21]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.6886903e-36 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.8837476e-28 3.6537768e-37 0.0000000e+00\n",
            "  3.2004907e-36]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.0000000e+00 1.0000000e+00 5.1565484e-28 1.5629632e-36 0.0000000e+00\n",
            "  5.0342883e-37]]\n",
            "[[1.3644180e-09 9.9999881e-01 1.1302695e-06 9.7801980e-08 2.0555802e-09\n",
            "  1.7900122e-08]]\n",
            "[[1.9718936e-13 1.0000000e+00 3.5111547e-08 9.8673986e-11 4.6458531e-14\n",
            "  2.1043260e-12]]\n",
            "[[9.8963454e-19 1.0000000e+00 2.0271100e-12 7.5708857e-17 1.8712798e-20\n",
            "  3.1320173e-16]]\n",
            "[[8.9654487e-38 1.0000000e+00 4.5943013e-26 1.2635332e-33 0.0000000e+00\n",
            "  6.7847564e-34]]\n",
            "[[4.9720855e-10 9.9999893e-01 5.7694677e-07 1.6551404e-08 5.5330157e-10\n",
            "  4.3894889e-07]]\n",
            "[[4.2006698e-33 1.0000000e+00 1.2978969e-24 2.6291586e-29 1.2604123e-35\n",
            "  7.7273169e-30]]\n",
            "[[8.1153073e-26 1.0000000e+00 4.8956867e-18 2.6845648e-22 8.9852499e-28\n",
            "  2.8714057e-23]]\n",
            "[[4.9780370e-19 1.0000000e+00 4.0215708e-14 4.6344709e-18 1.2696578e-21\n",
            "  3.5559349e-18]]\n",
            "[[0.0000000e+00 1.0000000e+00 4.7369476e-38 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[0.000000e+00 1.000000e+00 9.428771e-32 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[0. 1. 0. 0. 0. 0.]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.5960200e-30 0.0000000e+00 0.0000000e+00\n",
            "  1.3366781e-38]]\n",
            "[[3.7713468e-27 1.0000000e+00 1.8288891e-18 2.6121403e-24 1.0542507e-29\n",
            "  2.2289775e-25]]\n",
            "[[0.0000000e+00 1.0000000e+00 4.4963432e-32 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[2.5140305e-21 1.0000000e+00 1.0178694e-15 1.0331902e-20 1.5211913e-24\n",
            "  1.7627931e-20]]\n",
            "[[2.2416053e-33 1.0000000e+00 3.6956015e-24 1.1302992e-30 1.7841567e-36\n",
            "  6.0377434e-31]]\n",
            "[[0.0000000e+00 1.0000000e+00 1.2395898e-27 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00]]\n",
            "[[2.5967889e-14 1.0000000e+00 3.7210610e-11 2.9686862e-15 8.7556778e-19\n",
            "  3.0534144e-15]]\n",
            "[[4.1543829e-30 1.0000000e+00 1.7202174e-21 2.4102805e-27 1.9694903e-33\n",
            "  3.6632976e-28]]\n",
            "[[0.000000e+00 1.000000e+00 8.213925e-38 0.000000e+00 0.000000e+00\n",
            "  0.000000e+00]]\n",
            "[[4.43347992e-31 1.00000000e+00 3.46359112e-22 1.25768955e-27\n",
            "  1.23679923e-33 1.02054200e-27]]\n",
            "[[0.0000000e+00 1.0000000e+00 7.5121763e-30 3.2627044e-37 0.0000000e+00\n",
            "  8.6050703e-37]]\n",
            "[[0.0000000e+00 1.0000000e+00 5.3195623e-29 3.2419993e-36 0.0000000e+00\n",
            "  9.0021960e-36]]\n",
            "[[4.8004959e-35 1.0000000e+00 2.6358621e-25 1.8575844e-32 0.0000000e+00\n",
            "  2.1924629e-32]]\n",
            "[[1.1507855e-17 1.0000000e+00 2.1385367e-12 2.2752604e-14 1.6651779e-18\n",
            "  2.8026014e-15]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.4256647e-28 7.0661975e-37 0.0000000e+00\n",
            "  1.0432492e-36]]\n",
            "[[1.2583406e-22 1.0000000e+00 1.8732710e-14 1.0350211e-19 1.3410142e-23\n",
            "  4.7092649e-19]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.0966438e-29 1.4117658e-37 0.0000000e+00\n",
            "  8.1336049e-38]]\n",
            "[[6.3419672e-11 3.8095586e-05 4.6430944e-08 9.9996185e-01 3.1293981e-11\n",
            "  1.4908665e-10]]\n",
            "[[1.2507365e-14 1.0000000e+00 6.1465881e-11 2.7668796e-13 9.9736323e-17\n",
            "  2.1036027e-13]]\n",
            "[[1.8802852e-16 1.0000000e+00 2.1931586e-12 6.7375906e-17 2.6551658e-19\n",
            "  6.2223718e-16]]\n",
            "[[0.0000000e+00 1.0000000e+00 2.1439586e-29 1.3070552e-37 0.0000000e+00\n",
            "  8.1217959e-37]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0f8db215a1a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SCAAI_Rohit_volety_Problem_Statement_3/deep learning/disaster.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'//'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 314\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIvnqo68lzRq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stPLnClFN-zi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}