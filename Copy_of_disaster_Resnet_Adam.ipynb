{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy_of_disaster_Resnet_Adam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RV05/Classification-of-disasters-using-modified-architrctures/blob/main/Copy_of_disaster_Resnet_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01lAEMug9uHN",
        "outputId": "c8105512-4bdc-40f4-da9b-741b6ac927bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsYuPMHtVa8p",
        "outputId": "fac631fe-1bf1-4191-ba69-67019f9ad6fb"
      },
      "source": [
        "cd \"/content/drive/MyDrive/SCAAI_Rohit_volety_Problem_Statement_3/deep learning\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/deep learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT5oQ9GQegzo"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "import config\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "# \thelp=\"path to output loss/accuracy plot\")\n",
        "# args = vars(ap.parse_args())\n",
        "# # determine the total number of image paths in training, validation,\n",
        "# # and testing directories\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2W2oRSbfwYi"
      },
      "source": [
        "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
        "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
        "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
        "trainAug = ImageDataGenerator(\n",
        "\t# rotation_range=25,\n",
        "\t# zoom_range=0.1,\n",
        "\t# width_shift_range=0.1,\n",
        "\t# height_shift_range=0.1,\n",
        "\t# shear_range=0.2,\n",
        "\t# horizontal_flip=True,\n",
        "  # vertical_flip=True,\n",
        "\t# fill_mode=\"nearest\"\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWJ8ZBoSf8hg"
      },
      "source": [
        "# initialize the validation/testing data augmentation object (which\n",
        "# we'll be adding mean subtraction to)\n",
        "valAug = ImageDataGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lIkzyUDgAfo"
      },
      "source": [
        "# define the ImageNet mean subtraction (in RGB order) and set the\n",
        "# the mean subtraction value for each of the data augmentation\n",
        "# objects\n",
        "# mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "# trainAug.mean = mean\n",
        "# valAug.mean = mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph4f5GXagD-7",
        "outputId": "71fce5b5-8519-490a-8073-cffef37fa900"
      },
      "source": [
        "trainGen = trainAug.flow_from_directory(\n",
        "\tconfig.TRAIN_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=config.BS)\n",
        "# initialize the validation generator\n",
        "valGen = valAug.flow_from_directory(\n",
        "\tconfig.VAL_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)\n",
        "# initialize the testing generator\n",
        "testGen = valAug.flow_from_directory(\n",
        "\tconfig.TEST_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3309 images belonging to 6 classes.\n",
            "Found 31 images belonging to 6 classes.\n",
            "Found 1022 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4po5oj4TggC_",
        "outputId": "f8266c64-b383-4dec-9e23-6684d8cfa2c8"
      },
      "source": [
        "print(\"[INFO] preparing model...\")\n",
        "baseModel = ResNet101(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(200, 200, 3)))\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D()(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(len(config.CLASSES), activation=\"softmax\")(headModel)\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "# compile the model\n",
        "opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] preparing model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171450368/171446536 [==============================] - 2s 0us/step\n",
            "171458560/171446536 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7cBdl5Of3aG",
        "outputId": "9648569a-e16e-4ebb-eb34-88275562f48d"
      },
      "source": [
        "\n",
        "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit_generator(\n",
        "\ttrainGen,\n",
        "\tsteps_per_epoch=totalTrain // config.BS,\n",
        "\tvalidation_data=valGen,\n",
        "\tvalidation_steps=totalVal // config.BS,\n",
        "\tepochs=config.NUM_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "413/413 [==============================] - 1007s 2s/step - loss: 0.8976 - accuracy: 0.6731 - val_loss: 1.1096 - val_accuracy: 0.7083\n",
            "Epoch 2/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.3775 - accuracy: 0.8785 - val_loss: 1.2878 - val_accuracy: 0.7083\n",
            "Epoch 3/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.1887 - accuracy: 0.9373 - val_loss: 1.3568 - val_accuracy: 0.7083\n",
            "Epoch 4/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0959 - accuracy: 0.9703 - val_loss: 1.7730 - val_accuracy: 0.7083\n",
            "Epoch 5/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0673 - accuracy: 0.9758 - val_loss: 1.8799 - val_accuracy: 0.6667\n",
            "Epoch 6/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0881 - accuracy: 0.9727 - val_loss: 2.4740 - val_accuracy: 0.7083\n",
            "Epoch 7/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 2.5157 - val_accuracy: 0.7083\n",
            "Epoch 8/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0385 - accuracy: 0.9861 - val_loss: 2.7857 - val_accuracy: 0.7083\n",
            "Epoch 9/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0576 - accuracy: 0.9827 - val_loss: 2.5583 - val_accuracy: 0.7083\n",
            "Epoch 10/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0628 - accuracy: 0.9782 - val_loss: 3.1062 - val_accuracy: 0.6250\n",
            "Epoch 11/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0497 - accuracy: 0.9818 - val_loss: 2.2450 - val_accuracy: 0.6667\n",
            "Epoch 12/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0593 - accuracy: 0.9797 - val_loss: 2.6142 - val_accuracy: 0.7083\n",
            "Epoch 13/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0259 - accuracy: 0.9900 - val_loss: 2.4571 - val_accuracy: 0.7083\n",
            "Epoch 14/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0171 - accuracy: 0.9927 - val_loss: 2.6409 - val_accuracy: 0.7083\n",
            "Epoch 15/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0181 - accuracy: 0.9912 - val_loss: 2.8038 - val_accuracy: 0.7083\n",
            "Epoch 16/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0184 - accuracy: 0.9912 - val_loss: 2.2848 - val_accuracy: 0.7083\n",
            "Epoch 17/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0359 - accuracy: 0.9870 - val_loss: 2.5526 - val_accuracy: 0.7083\n",
            "Epoch 18/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0700 - accuracy: 0.9782 - val_loss: 1.9385 - val_accuracy: 0.7500\n",
            "Epoch 19/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0247 - accuracy: 0.9888 - val_loss: 3.1054 - val_accuracy: 0.7083\n",
            "Epoch 20/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0282 - accuracy: 0.9882 - val_loss: 2.6748 - val_accuracy: 0.7083\n",
            "Epoch 21/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0283 - accuracy: 0.9891 - val_loss: 1.3661 - val_accuracy: 0.7500\n",
            "Epoch 22/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0254 - accuracy: 0.9882 - val_loss: 2.5790 - val_accuracy: 0.7083\n",
            "Epoch 23/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0236 - accuracy: 0.9912 - val_loss: 2.3046 - val_accuracy: 0.7083\n",
            "Epoch 24/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0316 - accuracy: 0.9867 - val_loss: 2.8031 - val_accuracy: 0.7083\n",
            "Epoch 25/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0276 - accuracy: 0.9897 - val_loss: 3.2826 - val_accuracy: 0.7083\n",
            "Epoch 26/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0319 - accuracy: 0.9876 - val_loss: 2.9198 - val_accuracy: 0.7083\n",
            "Epoch 27/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0144 - accuracy: 0.9906 - val_loss: 2.9282 - val_accuracy: 0.7500\n",
            "Epoch 28/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0094 - accuracy: 0.9930 - val_loss: 3.3107 - val_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0129 - accuracy: 0.9933 - val_loss: 2.9605 - val_accuracy: 0.7500\n",
            "Epoch 30/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0098 - accuracy: 0.9924 - val_loss: 2.8125 - val_accuracy: 0.7083\n",
            "Epoch 31/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0245 - accuracy: 0.9921 - val_loss: 4.2387 - val_accuracy: 0.7083\n",
            "Epoch 32/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0723 - accuracy: 0.9803 - val_loss: 2.0725 - val_accuracy: 0.7083\n",
            "Epoch 33/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0192 - accuracy: 0.9897 - val_loss: 2.8038 - val_accuracy: 0.7500\n",
            "Epoch 34/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 2.5816 - val_accuracy: 0.7083\n",
            "Epoch 35/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0137 - accuracy: 0.9915 - val_loss: 2.7213 - val_accuracy: 0.7083\n",
            "Epoch 36/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0084 - accuracy: 0.9945 - val_loss: 2.7828 - val_accuracy: 0.7083\n",
            "Epoch 37/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0116 - accuracy: 0.9936 - val_loss: 2.7424 - val_accuracy: 0.7083\n",
            "Epoch 38/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0090 - accuracy: 0.9936 - val_loss: 3.0626 - val_accuracy: 0.7083\n",
            "Epoch 39/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0078 - accuracy: 0.9942 - val_loss: 3.0705 - val_accuracy: 0.7083\n",
            "Epoch 40/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0089 - accuracy: 0.9933 - val_loss: 2.8269 - val_accuracy: 0.7083\n",
            "Epoch 41/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 3.5713 - val_accuracy: 0.7083\n",
            "Epoch 42/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0441 - accuracy: 0.9842 - val_loss: 2.8653 - val_accuracy: 0.7083\n",
            "Epoch 43/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0465 - accuracy: 0.9815 - val_loss: 5.1117 - val_accuracy: 0.7083\n",
            "Epoch 44/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0419 - accuracy: 0.9870 - val_loss: 3.0102 - val_accuracy: 0.7083\n",
            "Epoch 45/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0114 - accuracy: 0.9933 - val_loss: 3.9526 - val_accuracy: 0.6667\n",
            "Epoch 46/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 3.7436 - val_accuracy: 0.7083\n",
            "Epoch 47/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0084 - accuracy: 0.9952 - val_loss: 3.4699 - val_accuracy: 0.7083\n",
            "Epoch 48/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0090 - accuracy: 0.9927 - val_loss: 3.5998 - val_accuracy: 0.7083\n",
            "Epoch 49/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0088 - accuracy: 0.9936 - val_loss: 3.6668 - val_accuracy: 0.7083\n",
            "Epoch 50/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0080 - accuracy: 0.9949 - val_loss: 3.7766 - val_accuracy: 0.7083\n",
            "Epoch 51/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0081 - accuracy: 0.9949 - val_loss: 3.6104 - val_accuracy: 0.7083\n",
            "Epoch 52/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0087 - accuracy: 0.9930 - val_loss: 4.2186 - val_accuracy: 0.7083\n",
            "Epoch 53/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0182 - accuracy: 0.9912 - val_loss: 4.6419 - val_accuracy: 0.6667\n",
            "Epoch 54/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0461 - accuracy: 0.9849 - val_loss: 3.7532 - val_accuracy: 0.7083\n",
            "Epoch 55/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0357 - accuracy: 0.9858 - val_loss: 3.8930 - val_accuracy: 0.7083\n",
            "Epoch 56/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0170 - accuracy: 0.9909 - val_loss: 3.7688 - val_accuracy: 0.7083\n",
            "Epoch 57/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0115 - accuracy: 0.9933 - val_loss: 3.9512 - val_accuracy: 0.7083\n",
            "Epoch 58/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0103 - accuracy: 0.9942 - val_loss: 4.2706 - val_accuracy: 0.7083\n",
            "Epoch 59/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0080 - accuracy: 0.9945 - val_loss: 3.5143 - val_accuracy: 0.7083\n",
            "Epoch 60/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9936 - val_loss: 4.1020 - val_accuracy: 0.7083\n",
            "Epoch 61/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0072 - accuracy: 0.9961 - val_loss: 4.1187 - val_accuracy: 0.7083\n",
            "Epoch 62/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0083 - accuracy: 0.9933 - val_loss: 4.1635 - val_accuracy: 0.7083\n",
            "Epoch 63/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0084 - accuracy: 0.9930 - val_loss: 3.8819 - val_accuracy: 0.7500\n",
            "Epoch 64/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0085 - accuracy: 0.9933 - val_loss: 4.1890 - val_accuracy: 0.7083\n",
            "Epoch 65/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0078 - accuracy: 0.9945 - val_loss: 3.9421 - val_accuracy: 0.7083\n",
            "Epoch 66/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0079 - accuracy: 0.9949 - val_loss: 4.0409 - val_accuracy: 0.7083\n",
            "Epoch 67/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0573 - accuracy: 0.9855 - val_loss: 3.6315 - val_accuracy: 0.7083\n",
            "Epoch 68/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0648 - accuracy: 0.9824 - val_loss: 3.5503 - val_accuracy: 0.7083\n",
            "Epoch 69/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0121 - accuracy: 0.9924 - val_loss: 3.4272 - val_accuracy: 0.7500\n",
            "Epoch 70/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0092 - accuracy: 0.9924 - val_loss: 4.0234 - val_accuracy: 0.7917\n",
            "Epoch 71/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9939 - val_loss: 4.0214 - val_accuracy: 0.7500\n",
            "Epoch 72/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0082 - accuracy: 0.9939 - val_loss: 4.2209 - val_accuracy: 0.7500\n",
            "Epoch 73/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 4.3688 - val_accuracy: 0.7500\n",
            "Epoch 74/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0083 - accuracy: 0.9949 - val_loss: 4.5234 - val_accuracy: 0.7500\n",
            "Epoch 75/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0088 - accuracy: 0.9918 - val_loss: 4.6049 - val_accuracy: 0.7500\n",
            "Epoch 76/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9933 - val_loss: 4.7294 - val_accuracy: 0.7500\n",
            "Epoch 77/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0076 - accuracy: 0.9942 - val_loss: 5.4704 - val_accuracy: 0.7083\n",
            "Epoch 78/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9933 - val_loss: 4.8490 - val_accuracy: 0.7917\n",
            "Epoch 79/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0079 - accuracy: 0.9939 - val_loss: 5.2302 - val_accuracy: 0.7500\n",
            "Epoch 80/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9936 - val_loss: 5.1384 - val_accuracy: 0.7083\n",
            "Epoch 81/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0128 - accuracy: 0.9924 - val_loss: 4.4119 - val_accuracy: 0.7083\n",
            "Epoch 82/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.1019 - accuracy: 0.9788 - val_loss: 3.4661 - val_accuracy: 0.7083\n",
            "Epoch 83/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0177 - accuracy: 0.9921 - val_loss: 4.6548 - val_accuracy: 0.6667\n",
            "Epoch 84/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0233 - accuracy: 0.9903 - val_loss: 6.4323 - val_accuracy: 0.7083\n",
            "Epoch 85/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0084 - accuracy: 0.9936 - val_loss: 5.5897 - val_accuracy: 0.6667\n",
            "Epoch 86/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0081 - accuracy: 0.9936 - val_loss: 5.6541 - val_accuracy: 0.6667\n",
            "Epoch 87/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 5.9407 - val_accuracy: 0.6667\n",
            "Epoch 88/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0102 - accuracy: 0.9945 - val_loss: 5.6171 - val_accuracy: 0.6667\n",
            "Epoch 89/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0077 - accuracy: 0.9961 - val_loss: 5.7783 - val_accuracy: 0.6667\n",
            "Epoch 90/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9924 - val_loss: 5.7325 - val_accuracy: 0.6667\n",
            "Epoch 91/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9958 - val_loss: 5.8044 - val_accuracy: 0.6667\n",
            "Epoch 92/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0077 - accuracy: 0.9942 - val_loss: 5.9375 - val_accuracy: 0.6667\n",
            "Epoch 93/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 5.8013 - val_accuracy: 0.6667\n",
            "Epoch 94/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9936 - val_loss: 5.7982 - val_accuracy: 0.6667\n",
            "Epoch 95/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9933 - val_loss: 6.2370 - val_accuracy: 0.6667\n",
            "Epoch 96/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9930 - val_loss: 6.0551 - val_accuracy: 0.6667\n",
            "Epoch 97/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9936 - val_loss: 6.1545 - val_accuracy: 0.6667\n",
            "Epoch 98/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0087 - accuracy: 0.9939 - val_loss: 7.4926 - val_accuracy: 0.7083\n",
            "Epoch 99/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0524 - accuracy: 0.9846 - val_loss: 4.9534 - val_accuracy: 0.7083\n",
            "Epoch 100/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0204 - accuracy: 0.9921 - val_loss: 12.1993 - val_accuracy: 0.6250\n",
            "Epoch 101/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0381 - accuracy: 0.9858 - val_loss: 4.9819 - val_accuracy: 0.7083\n",
            "Epoch 102/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0096 - accuracy: 0.9939 - val_loss: 4.9070 - val_accuracy: 0.6667\n",
            "Epoch 103/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0101 - accuracy: 0.9912 - val_loss: 5.4899 - val_accuracy: 0.6667\n",
            "Epoch 104/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0078 - accuracy: 0.9949 - val_loss: 5.3904 - val_accuracy: 0.6667\n",
            "Epoch 105/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0076 - accuracy: 0.9936 - val_loss: 5.6529 - val_accuracy: 0.7083\n",
            "Epoch 106/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9936 - val_loss: 5.6955 - val_accuracy: 0.7083\n",
            "Epoch 107/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9930 - val_loss: 5.6849 - val_accuracy: 0.7083\n",
            "Epoch 108/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0099 - accuracy: 0.9936 - val_loss: 5.8166 - val_accuracy: 0.7083\n",
            "Epoch 109/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0095 - accuracy: 0.9933 - val_loss: 5.6578 - val_accuracy: 0.7083\n",
            "Epoch 110/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9936 - val_loss: 5.7385 - val_accuracy: 0.7083\n",
            "Epoch 111/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9936 - val_loss: 6.0804 - val_accuracy: 0.7083\n",
            "Epoch 112/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0076 - accuracy: 0.9942 - val_loss: 6.2607 - val_accuracy: 0.7083\n",
            "Epoch 113/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0078 - accuracy: 0.9939 - val_loss: 6.4074 - val_accuracy: 0.7083\n",
            "Epoch 114/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9933 - val_loss: 6.3583 - val_accuracy: 0.7083\n",
            "Epoch 115/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0082 - accuracy: 0.9952 - val_loss: 6.5647 - val_accuracy: 0.7083\n",
            "Epoch 116/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0548 - accuracy: 0.9849 - val_loss: 3.0507 - val_accuracy: 0.6667\n",
            "Epoch 117/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0190 - accuracy: 0.9909 - val_loss: 7.1164 - val_accuracy: 0.7083\n",
            "Epoch 118/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0117 - accuracy: 0.9942 - val_loss: 6.6556 - val_accuracy: 0.7500\n",
            "Epoch 119/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0312 - accuracy: 0.9906 - val_loss: 4.5868 - val_accuracy: 0.7083\n",
            "Epoch 120/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0085 - accuracy: 0.9930 - val_loss: 5.1220 - val_accuracy: 0.7083\n",
            "Epoch 121/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9942 - val_loss: 5.7488 - val_accuracy: 0.7083\n",
            "Epoch 122/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0078 - accuracy: 0.9942 - val_loss: 5.3376 - val_accuracy: 0.6667\n",
            "Epoch 123/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0077 - accuracy: 0.9945 - val_loss: 5.5478 - val_accuracy: 0.7083\n",
            "Epoch 124/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0076 - accuracy: 0.9952 - val_loss: 5.6673 - val_accuracy: 0.7083\n",
            "Epoch 125/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0078 - accuracy: 0.9945 - val_loss: 5.6717 - val_accuracy: 0.7083\n",
            "Epoch 126/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9939 - val_loss: 6.2826 - val_accuracy: 0.7083\n",
            "Epoch 127/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9930 - val_loss: 6.0387 - val_accuracy: 0.7083\n",
            "Epoch 128/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0092 - accuracy: 0.9955 - val_loss: 7.4357 - val_accuracy: 0.7083\n",
            "Epoch 129/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9933 - val_loss: 7.6424 - val_accuracy: 0.7083\n",
            "Epoch 130/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0085 - accuracy: 0.9952 - val_loss: 7.1220 - val_accuracy: 0.7083\n",
            "Epoch 131/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0084 - accuracy: 0.9942 - val_loss: 7.0361 - val_accuracy: 0.7083\n",
            "Epoch 132/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0078 - accuracy: 0.9949 - val_loss: 6.5034 - val_accuracy: 0.7083\n",
            "Epoch 133/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0076 - accuracy: 0.9961 - val_loss: 7.0700 - val_accuracy: 0.7083\n",
            "Epoch 134/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9933 - val_loss: 6.7304 - val_accuracy: 0.7083\n",
            "Epoch 135/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 6.8378 - val_accuracy: 0.6667\n",
            "Epoch 136/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0553 - accuracy: 0.9824 - val_loss: 9.8347 - val_accuracy: 0.6667\n",
            "Epoch 137/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0297 - accuracy: 0.9918 - val_loss: 6.2735 - val_accuracy: 0.6667\n",
            "Epoch 138/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0081 - accuracy: 0.9939 - val_loss: 7.8944 - val_accuracy: 0.6667\n",
            "Epoch 139/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0077 - accuracy: 0.9936 - val_loss: 8.7162 - val_accuracy: 0.6667\n",
            "Epoch 140/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 8.7198 - val_accuracy: 0.6667\n",
            "Epoch 141/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0076 - accuracy: 0.9949 - val_loss: 8.6420 - val_accuracy: 0.6667\n",
            "Epoch 142/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9952 - val_loss: 8.6748 - val_accuracy: 0.6667\n",
            "Epoch 143/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0078 - accuracy: 0.9952 - val_loss: 8.6384 - val_accuracy: 0.6667\n",
            "Epoch 144/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0082 - accuracy: 0.9936 - val_loss: 8.5334 - val_accuracy: 0.6667\n",
            "Epoch 145/200\n",
            "413/413 [==============================] - 27s 67ms/step - loss: 0.0086 - accuracy: 0.9936 - val_loss: 9.4077 - val_accuracy: 0.6667\n",
            "Epoch 146/200\n",
            "413/413 [==============================] - 27s 65ms/step - loss: 0.0080 - accuracy: 0.9924 - val_loss: 9.3690 - val_accuracy: 0.6667\n",
            "Epoch 147/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0078 - accuracy: 0.9949 - val_loss: 8.7795 - val_accuracy: 0.6667\n",
            "Epoch 148/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0076 - accuracy: 0.9949 - val_loss: 10.1603 - val_accuracy: 0.6667\n",
            "Epoch 149/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0082 - accuracy: 0.9936 - val_loss: 8.7166 - val_accuracy: 0.6667\n",
            "Epoch 150/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0078 - accuracy: 0.9936 - val_loss: 8.9200 - val_accuracy: 0.6667\n",
            "Epoch 151/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0077 - accuracy: 0.9939 - val_loss: 9.0168 - val_accuracy: 0.6667\n",
            "Epoch 152/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0078 - accuracy: 0.9930 - val_loss: 9.8535 - val_accuracy: 0.6667\n",
            "Epoch 153/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0076 - accuracy: 0.9945 - val_loss: 9.6234 - val_accuracy: 0.6667\n",
            "Epoch 154/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 10.7868 - val_accuracy: 0.6667\n",
            "Epoch 155/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0092 - accuracy: 0.9955 - val_loss: 14.7890 - val_accuracy: 0.6667\n",
            "Epoch 156/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0767 - accuracy: 0.9815 - val_loss: 6.2820 - val_accuracy: 0.7083\n",
            "Epoch 157/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0224 - accuracy: 0.9912 - val_loss: 9.4950 - val_accuracy: 0.7083\n",
            "Epoch 158/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0268 - accuracy: 0.9894 - val_loss: 6.8825 - val_accuracy: 0.7083\n",
            "Epoch 159/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0081 - accuracy: 0.9933 - val_loss: 6.0971 - val_accuracy: 0.7083\n",
            "Epoch 160/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0088 - accuracy: 0.9936 - val_loss: 7.1054 - val_accuracy: 0.7083\n",
            "Epoch 161/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 9.3415 - val_accuracy: 0.7083\n",
            "Epoch 162/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0078 - accuracy: 0.9942 - val_loss: 9.4349 - val_accuracy: 0.7083\n",
            "Epoch 163/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0083 - accuracy: 0.9930 - val_loss: 9.4877 - val_accuracy: 0.7083\n",
            "Epoch 164/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0081 - accuracy: 0.9939 - val_loss: 9.2068 - val_accuracy: 0.7083\n",
            "Epoch 165/200\n",
            "413/413 [==============================] - 27s 67ms/step - loss: 0.0079 - accuracy: 0.9945 - val_loss: 9.1128 - val_accuracy: 0.7083\n",
            "Epoch 166/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0072 - accuracy: 0.9958 - val_loss: 9.3456 - val_accuracy: 0.7083\n",
            "Epoch 167/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0077 - accuracy: 0.9952 - val_loss: 9.2805 - val_accuracy: 0.7083\n",
            "Epoch 168/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0078 - accuracy: 0.9942 - val_loss: 9.1480 - val_accuracy: 0.7083\n",
            "Epoch 169/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0075 - accuracy: 0.9961 - val_loss: 8.7092 - val_accuracy: 0.7083\n",
            "Epoch 170/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 8.7873 - val_accuracy: 0.7083\n",
            "Epoch 171/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0076 - accuracy: 0.9942 - val_loss: 9.2175 - val_accuracy: 0.7083\n",
            "Epoch 172/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0079 - accuracy: 0.9949 - val_loss: 9.2749 - val_accuracy: 0.7083\n",
            "Epoch 173/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.1097 - accuracy: 0.9861 - val_loss: 5.2702 - val_accuracy: 0.6667\n",
            "Epoch 174/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0323 - accuracy: 0.9921 - val_loss: 6.7296 - val_accuracy: 0.7083\n",
            "Epoch 175/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0125 - accuracy: 0.9927 - val_loss: 7.6589 - val_accuracy: 0.7083\n",
            "Epoch 176/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0084 - accuracy: 0.9942 - val_loss: 8.7867 - val_accuracy: 0.7083\n",
            "Epoch 177/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0080 - accuracy: 0.9942 - val_loss: 7.0020 - val_accuracy: 0.7083\n",
            "Epoch 178/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0077 - accuracy: 0.9942 - val_loss: 7.9376 - val_accuracy: 0.7083\n",
            "Epoch 179/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0081 - accuracy: 0.9927 - val_loss: 8.1708 - val_accuracy: 0.7083\n",
            "Epoch 180/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0078 - accuracy: 0.9936 - val_loss: 8.4536 - val_accuracy: 0.7083\n",
            "Epoch 181/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0080 - accuracy: 0.9930 - val_loss: 8.8267 - val_accuracy: 0.7083\n",
            "Epoch 182/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0076 - accuracy: 0.9942 - val_loss: 9.1690 - val_accuracy: 0.7083\n",
            "Epoch 183/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0087 - accuracy: 0.9942 - val_loss: 11.6065 - val_accuracy: 0.7083\n",
            "Epoch 184/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 7.6702 - val_accuracy: 0.7083\n",
            "Epoch 185/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0078 - accuracy: 0.9939 - val_loss: 11.3223 - val_accuracy: 0.7083\n",
            "Epoch 186/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0131 - accuracy: 0.9927 - val_loss: 11.0154 - val_accuracy: 0.7083\n",
            "Epoch 187/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0437 - accuracy: 0.9924 - val_loss: 8.4414 - val_accuracy: 0.7083\n",
            "Epoch 188/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0112 - accuracy: 0.9942 - val_loss: 8.2144 - val_accuracy: 0.7083\n",
            "Epoch 189/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0194 - accuracy: 0.9915 - val_loss: 6.6026 - val_accuracy: 0.7083\n",
            "Epoch 190/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0107 - accuracy: 0.9924 - val_loss: 9.2169 - val_accuracy: 0.6667\n",
            "Epoch 191/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0080 - accuracy: 0.9942 - val_loss: 9.5185 - val_accuracy: 0.6667\n",
            "Epoch 192/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0077 - accuracy: 0.9955 - val_loss: 9.2994 - val_accuracy: 0.6667\n",
            "Epoch 193/200\n",
            "413/413 [==============================] - 27s 66ms/step - loss: 0.0077 - accuracy: 0.9939 - val_loss: 9.3417 - val_accuracy: 0.6667\n",
            "Epoch 194/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0079 - accuracy: 0.9933 - val_loss: 9.9108 - val_accuracy: 0.6667\n",
            "Epoch 195/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0079 - accuracy: 0.9936 - val_loss: 10.2377 - val_accuracy: 0.6667\n",
            "Epoch 196/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0079 - accuracy: 0.9924 - val_loss: 11.1292 - val_accuracy: 0.6667\n",
            "Epoch 197/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0077 - accuracy: 0.9942 - val_loss: 11.1311 - val_accuracy: 0.6667\n",
            "Epoch 198/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.0076 - accuracy: 0.9952 - val_loss: 11.1674 - val_accuracy: 0.6667\n",
            "Epoch 199/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0079 - accuracy: 0.9930 - val_loss: 11.8609 - val_accuracy: 0.6667\n",
            "Epoch 200/200\n",
            "413/413 [==============================] - 28s 67ms/step - loss: 0.0077 - accuracy: 0.9936 - val_loss: 12.2278 - val_accuracy: 0.6667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj41CXepgq1Q",
        "outputId": "31c30795-bc08-4055-f001-b16226bf1c74"
      },
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "testGen.reset()\n",
        "predIdxs = model.predict_generator(testGen,\n",
        "\tsteps=(totalTest // config.BS) + 1)\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testGen.classes, predIdxs,\n",
        "\ttarget_names=testGen.class_indices.keys()))\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
        "N = config.NUM_EPOCHS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fire       0.89      0.79      0.83       200\n",
            "       human       0.84      0.76      0.80        42\n",
            "       infra       0.43      0.87      0.57       210\n",
            "        land       0.76      0.48      0.59       148\n",
            "    nodamage       0.25      0.04      0.07       211\n",
            "       water       0.68      0.82      0.74       211\n",
            "\n",
            "    accuracy                           0.61      1022\n",
            "   macro avg       0.64      0.63      0.60      1022\n",
            "weighted avg       0.60      0.61      0.57      1022\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUGR_HTRgNhC"
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"Resnet_adam_plot.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEcaVZFjhvEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90dfd00f-c6a2-4006-c445-5d20dde29f9f"
      },
      "source": [
        "H.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.6731293797492981,\n",
              "  0.8785216808319092,\n",
              "  0.9372917413711548,\n",
              "  0.9703119993209839,\n",
              "  0.9757649302482605,\n",
              "  0.9727355241775513,\n",
              "  0.9794001579284668,\n",
              "  0.9860648512840271,\n",
              "  0.9827325344085693,\n",
              "  0.9781884551048279,\n",
              "  0.9818236827850342,\n",
              "  0.9797031283378601,\n",
              "  0.9900030493736267,\n",
              "  0.9927294850349426,\n",
              "  0.9912148118019104,\n",
              "  0.9912148118019104,\n",
              "  0.9869736433029175,\n",
              "  0.9781884551048279,\n",
              "  0.988791286945343,\n",
              "  0.9881854057312012,\n",
              "  0.9890941977500916,\n",
              "  0.9881854057312012,\n",
              "  0.9912148118019104,\n",
              "  0.986670732498169,\n",
              "  0.9897000789642334,\n",
              "  0.9875795245170593,\n",
              "  0.9906089305877686,\n",
              "  0.9930323958396912,\n",
              "  0.9933353662490845,\n",
              "  0.9924265146255493,\n",
              "  0.9921236038208008,\n",
              "  0.980309009552002,\n",
              "  0.9897000789642334,\n",
              "  0.9918206334114075,\n",
              "  0.9915177226066589,\n",
              "  0.9945471286773682,\n",
              "  0.993638277053833,\n",
              "  0.993638277053833,\n",
              "  0.9942441582679749,\n",
              "  0.9933353662490845,\n",
              "  0.9954559206962585,\n",
              "  0.9842472076416016,\n",
              "  0.9815207719802856,\n",
              "  0.9869736433029175,\n",
              "  0.9933353662490845,\n",
              "  0.9918206334114075,\n",
              "  0.99515300989151,\n",
              "  0.9927294850349426,\n",
              "  0.993638277053833,\n",
              "  0.9948500394821167,\n",
              "  0.9948500394821167,\n",
              "  0.9930323958396912,\n",
              "  0.9912148118019104,\n",
              "  0.9848530888557434,\n",
              "  0.9857618808746338,\n",
              "  0.9909118413925171,\n",
              "  0.9933353662490845,\n",
              "  0.9942441582679749,\n",
              "  0.9945471286773682,\n",
              "  0.993638277053833,\n",
              "  0.9960618019104004,\n",
              "  0.9933353662490845,\n",
              "  0.9930323958396912,\n",
              "  0.9933353662490845,\n",
              "  0.9945471286773682,\n",
              "  0.9948500394821167,\n",
              "  0.9854589700698853,\n",
              "  0.982429563999176,\n",
              "  0.9924265146255493,\n",
              "  0.9924265146255493,\n",
              "  0.9939412474632263,\n",
              "  0.9939412474632263,\n",
              "  0.9942441582679749,\n",
              "  0.9948500394821167,\n",
              "  0.9918206334114075,\n",
              "  0.9933353662490845,\n",
              "  0.9942441582679749,\n",
              "  0.9933353662490845,\n",
              "  0.9939412474632263,\n",
              "  0.993638277053833,\n",
              "  0.9924265146255493,\n",
              "  0.978794276714325,\n",
              "  0.9921236038208008,\n",
              "  0.9903059601783752,\n",
              "  0.993638277053833,\n",
              "  0.993638277053833,\n",
              "  0.9942441582679749,\n",
              "  0.9945471286773682,\n",
              "  0.9960618019104004,\n",
              "  0.9924265146255493,\n",
              "  0.9957588315010071,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.993638277053833,\n",
              "  0.9933353662490845,\n",
              "  0.9930323958396912,\n",
              "  0.993638277053833,\n",
              "  0.9939412474632263,\n",
              "  0.9845501184463501,\n",
              "  0.9921236038208008,\n",
              "  0.9857618808746338,\n",
              "  0.9939412474632263,\n",
              "  0.9912148118019104,\n",
              "  0.9948500394821167,\n",
              "  0.993638277053833,\n",
              "  0.993638277053833,\n",
              "  0.9930323958396912,\n",
              "  0.993638277053833,\n",
              "  0.9933353662490845,\n",
              "  0.993638277053833,\n",
              "  0.993638277053833,\n",
              "  0.9942441582679749,\n",
              "  0.9939412474632263,\n",
              "  0.9933353662490845,\n",
              "  0.99515300989151,\n",
              "  0.9848530888557434,\n",
              "  0.9909118413925171,\n",
              "  0.9942441582679749,\n",
              "  0.9906089305877686,\n",
              "  0.9930323958396912,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9945471286773682,\n",
              "  0.99515300989151,\n",
              "  0.9945471286773682,\n",
              "  0.9939412474632263,\n",
              "  0.9930323958396912,\n",
              "  0.9954559206962585,\n",
              "  0.9933353662490845,\n",
              "  0.99515300989151,\n",
              "  0.9942441582679749,\n",
              "  0.9948500394821167,\n",
              "  0.9960618019104004,\n",
              "  0.9933353662490845,\n",
              "  0.9933353662490845,\n",
              "  0.982429563999176,\n",
              "  0.9918206334114075,\n",
              "  0.9939412474632263,\n",
              "  0.993638277053833,\n",
              "  0.9942441582679749,\n",
              "  0.9948500394821167,\n",
              "  0.99515300989151,\n",
              "  0.99515300989151,\n",
              "  0.993638277053833,\n",
              "  0.993638277053833,\n",
              "  0.9924265146255493,\n",
              "  0.9948500394821167,\n",
              "  0.9948500394821167,\n",
              "  0.993638277053833,\n",
              "  0.993638277053833,\n",
              "  0.9939412474632263,\n",
              "  0.9930323958396912,\n",
              "  0.9945471286773682,\n",
              "  0.9942441582679749,\n",
              "  0.9954559206962585,\n",
              "  0.9815207719802856,\n",
              "  0.9912148118019104,\n",
              "  0.9893971681594849,\n",
              "  0.9933353662490845,\n",
              "  0.993638277053833,\n",
              "  0.9948500394821167,\n",
              "  0.9942441582679749,\n",
              "  0.9930323958396912,\n",
              "  0.9939412474632263,\n",
              "  0.9945471286773682,\n",
              "  0.9957588315010071,\n",
              "  0.99515300989151,\n",
              "  0.9942441582679749,\n",
              "  0.9960618019104004,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9948500394821167,\n",
              "  0.9860648512840271,\n",
              "  0.9921236038208008,\n",
              "  0.9927294850349426,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9927294850349426,\n",
              "  0.993638277053833,\n",
              "  0.9930323958396912,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9948500394821167,\n",
              "  0.9939412474632263,\n",
              "  0.9927294850349426,\n",
              "  0.9924265146255493,\n",
              "  0.9942441582679749,\n",
              "  0.9915177226066589,\n",
              "  0.9924265146255493,\n",
              "  0.9942441582679749,\n",
              "  0.9954559206962585,\n",
              "  0.9939412474632263,\n",
              "  0.9933353662490845,\n",
              "  0.993638277053833,\n",
              "  0.9924265146255493,\n",
              "  0.9942441582679749,\n",
              "  0.99515300989151,\n",
              "  0.9930323958396912,\n",
              "  0.993638277053833],\n",
              " 'loss': [0.8975791931152344,\n",
              "  0.3774673640727997,\n",
              "  0.1886989027261734,\n",
              "  0.09587778896093369,\n",
              "  0.06726478040218353,\n",
              "  0.08814298361539841,\n",
              "  0.05784999206662178,\n",
              "  0.0385015569627285,\n",
              "  0.05762115493416786,\n",
              "  0.06281261891126633,\n",
              "  0.04973972216248512,\n",
              "  0.059260278940200806,\n",
              "  0.0259279515594244,\n",
              "  0.01713835820555687,\n",
              "  0.01814217120409012,\n",
              "  0.018405869603157043,\n",
              "  0.0358794741332531,\n",
              "  0.070002980530262,\n",
              "  0.024737153202295303,\n",
              "  0.028206370770931244,\n",
              "  0.02830883115530014,\n",
              "  0.025360824540257454,\n",
              "  0.02363460324704647,\n",
              "  0.03163716197013855,\n",
              "  0.027639521285891533,\n",
              "  0.031922176480293274,\n",
              "  0.01442986074835062,\n",
              "  0.009359162300825119,\n",
              "  0.012883317656815052,\n",
              "  0.009752243757247925,\n",
              "  0.024456607177853584,\n",
              "  0.07234977930784225,\n",
              "  0.019234731793403625,\n",
              "  0.019710930064320564,\n",
              "  0.013685878366231918,\n",
              "  0.008389463648200035,\n",
              "  0.01163453422486782,\n",
              "  0.00895385630428791,\n",
              "  0.007829778827726841,\n",
              "  0.00890075322240591,\n",
              "  0.011058945208787918,\n",
              "  0.044064123183488846,\n",
              "  0.04652126878499985,\n",
              "  0.04188474640250206,\n",
              "  0.011425227858126163,\n",
              "  0.013593325391411781,\n",
              "  0.008380142971873283,\n",
              "  0.008989774622023106,\n",
              "  0.008753170259296894,\n",
              "  0.00795633252710104,\n",
              "  0.00814990233629942,\n",
              "  0.008652005344629288,\n",
              "  0.018227437511086464,\n",
              "  0.046097226440906525,\n",
              "  0.03571614995598793,\n",
              "  0.016984669491648674,\n",
              "  0.011532903648912907,\n",
              "  0.010311434976756573,\n",
              "  0.007956516928970814,\n",
              "  0.00812884233891964,\n",
              "  0.007202501874417067,\n",
              "  0.008336239494383335,\n",
              "  0.008352434262633324,\n",
              "  0.008505349047482014,\n",
              "  0.007808764465153217,\n",
              "  0.007949323393404484,\n",
              "  0.057298824191093445,\n",
              "  0.06482775509357452,\n",
              "  0.01213572733104229,\n",
              "  0.009240643121302128,\n",
              "  0.008009394630789757,\n",
              "  0.008154258131980896,\n",
              "  0.007915757596492767,\n",
              "  0.008341417647898197,\n",
              "  0.00875089317560196,\n",
              "  0.00810914020985365,\n",
              "  0.007620351854711771,\n",
              "  0.008126135915517807,\n",
              "  0.007879193872213364,\n",
              "  0.008141866885125637,\n",
              "  0.012833869084715843,\n",
              "  0.10193338990211487,\n",
              "  0.017734000459313393,\n",
              "  0.023262344300746918,\n",
              "  0.008428731933236122,\n",
              "  0.00813246052712202,\n",
              "  0.007886052131652832,\n",
              "  0.01019514910876751,\n",
              "  0.0077236793003976345,\n",
              "  0.008249140344560146,\n",
              "  0.007947565987706184,\n",
              "  0.00771681871265173,\n",
              "  0.007932081818580627,\n",
              "  0.007989278063178062,\n",
              "  0.008185798302292824,\n",
              "  0.008174609392881393,\n",
              "  0.00804105494171381,\n",
              "  0.008661875501275063,\n",
              "  0.052368778735399246,\n",
              "  0.020431162789463997,\n",
              "  0.038137417286634445,\n",
              "  0.009609507396817207,\n",
              "  0.010114072822034359,\n",
              "  0.0077981725335121155,\n",
              "  0.007648398634046316,\n",
              "  0.008020560257136822,\n",
              "  0.007957269437611103,\n",
              "  0.009853296913206577,\n",
              "  0.009521703235805035,\n",
              "  0.00792902521789074,\n",
              "  0.007873986847698689,\n",
              "  0.00764782540500164,\n",
              "  0.007755700033158064,\n",
              "  0.008082946762442589,\n",
              "  0.008182902820408344,\n",
              "  0.05478358641266823,\n",
              "  0.018991483375430107,\n",
              "  0.011686068028211594,\n",
              "  0.031198017299175262,\n",
              "  0.00850763451308012,\n",
              "  0.008167868480086327,\n",
              "  0.007787467446178198,\n",
              "  0.007748443633317947,\n",
              "  0.007611499167978764,\n",
              "  0.007775302976369858,\n",
              "  0.008209830150008202,\n",
              "  0.007940690033137798,\n",
              "  0.009153904393315315,\n",
              "  0.008227449841797352,\n",
              "  0.008483866229653358,\n",
              "  0.008437840268015862,\n",
              "  0.007839858531951904,\n",
              "  0.007621110416948795,\n",
              "  0.00796031765639782,\n",
              "  0.02039196901023388,\n",
              "  0.05533916875720024,\n",
              "  0.029680339619517326,\n",
              "  0.008137230761349201,\n",
              "  0.007722542621195316,\n",
              "  0.007885726168751717,\n",
              "  0.0075924634002149105,\n",
              "  0.00806642696261406,\n",
              "  0.0078096273355185986,\n",
              "  0.00819021463394165,\n",
              "  0.008578031323850155,\n",
              "  0.008000321686267853,\n",
              "  0.007789629511535168,\n",
              "  0.0075643775053322315,\n",
              "  0.008163000456988811,\n",
              "  0.007751317694783211,\n",
              "  0.00770151149481535,\n",
              "  0.007800236344337463,\n",
              "  0.007625068537890911,\n",
              "  0.007897617295384407,\n",
              "  0.009221049025654793,\n",
              "  0.0767141729593277,\n",
              "  0.022355597466230392,\n",
              "  0.02683642879128456,\n",
              "  0.008095304481685162,\n",
              "  0.00875562522560358,\n",
              "  0.015738874673843384,\n",
              "  0.007822003215551376,\n",
              "  0.008284377865493298,\n",
              "  0.008073403500020504,\n",
              "  0.007941492833197117,\n",
              "  0.0072222077287733555,\n",
              "  0.00770496716722846,\n",
              "  0.007837625220417976,\n",
              "  0.007542775012552738,\n",
              "  0.007919288240373135,\n",
              "  0.007616884540766478,\n",
              "  0.007853141985833645,\n",
              "  0.10966400057077408,\n",
              "  0.03227382153272629,\n",
              "  0.012534817680716515,\n",
              "  0.008378230035305023,\n",
              "  0.008010992780327797,\n",
              "  0.007748524192720652,\n",
              "  0.008054814301431179,\n",
              "  0.007803958375006914,\n",
              "  0.00799894891679287,\n",
              "  0.00764722004532814,\n",
              "  0.008675646968185902,\n",
              "  0.014439436607062817,\n",
              "  0.007813932374119759,\n",
              "  0.013062718324363232,\n",
              "  0.04369061067700386,\n",
              "  0.011193069629371166,\n",
              "  0.019391758367419243,\n",
              "  0.010658763349056244,\n",
              "  0.007953234948217869,\n",
              "  0.0076980083249509335,\n",
              "  0.007721975911408663,\n",
              "  0.007884401828050613,\n",
              "  0.00794529914855957,\n",
              "  0.007945980876684189,\n",
              "  0.0076520005241036415,\n",
              "  0.007616196293383837,\n",
              "  0.007928824983537197,\n",
              "  0.007733134552836418],\n",
              " 'val_accuracy': [0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.7916666865348816,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7916666865348816,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.625,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816],\n",
              " 'val_loss': [1.1096091270446777,\n",
              "  1.2878258228302002,\n",
              "  1.3568023443222046,\n",
              "  1.7729653120040894,\n",
              "  1.8799387216567993,\n",
              "  2.474017858505249,\n",
              "  2.5157296657562256,\n",
              "  2.7856998443603516,\n",
              "  2.5583126544952393,\n",
              "  3.1062307357788086,\n",
              "  2.2450411319732666,\n",
              "  2.614187240600586,\n",
              "  2.4571053981781006,\n",
              "  2.6408846378326416,\n",
              "  2.8037612438201904,\n",
              "  2.2848005294799805,\n",
              "  2.5525808334350586,\n",
              "  1.9385372400283813,\n",
              "  3.105417013168335,\n",
              "  2.6748266220092773,\n",
              "  1.3660974502563477,\n",
              "  2.579045295715332,\n",
              "  2.3046038150787354,\n",
              "  2.8031415939331055,\n",
              "  3.28262996673584,\n",
              "  2.9198198318481445,\n",
              "  2.9282257556915283,\n",
              "  3.3107259273529053,\n",
              "  2.960522413253784,\n",
              "  2.812485456466675,\n",
              "  4.238678932189941,\n",
              "  2.0724823474884033,\n",
              "  2.803753137588501,\n",
              "  2.5816237926483154,\n",
              "  2.721294403076172,\n",
              "  2.782801389694214,\n",
              "  2.7423620223999023,\n",
              "  3.062586784362793,\n",
              "  3.070509672164917,\n",
              "  2.8268659114837646,\n",
              "  3.571345567703247,\n",
              "  2.8653299808502197,\n",
              "  5.111709117889404,\n",
              "  3.010150909423828,\n",
              "  3.9526185989379883,\n",
              "  3.743572235107422,\n",
              "  3.469888687133789,\n",
              "  3.59975528717041,\n",
              "  3.6668121814727783,\n",
              "  3.77663516998291,\n",
              "  3.6103994846343994,\n",
              "  4.218569278717041,\n",
              "  4.641870498657227,\n",
              "  3.7531659603118896,\n",
              "  3.893008232116699,\n",
              "  3.768777847290039,\n",
              "  3.951158285140991,\n",
              "  4.2705769538879395,\n",
              "  3.51432728767395,\n",
              "  4.101969242095947,\n",
              "  4.118675708770752,\n",
              "  4.16350793838501,\n",
              "  3.8818652629852295,\n",
              "  4.189041614532471,\n",
              "  3.9420711994171143,\n",
              "  4.0409159660339355,\n",
              "  3.63147234916687,\n",
              "  3.5502872467041016,\n",
              "  3.4271984100341797,\n",
              "  4.023416996002197,\n",
              "  4.02144193649292,\n",
              "  4.220928192138672,\n",
              "  4.368833065032959,\n",
              "  4.523449420928955,\n",
              "  4.604878902435303,\n",
              "  4.729353904724121,\n",
              "  5.470373630523682,\n",
              "  4.8490424156188965,\n",
              "  5.230227947235107,\n",
              "  5.138352870941162,\n",
              "  4.411884307861328,\n",
              "  3.4661409854888916,\n",
              "  4.654840469360352,\n",
              "  6.432290554046631,\n",
              "  5.589745998382568,\n",
              "  5.654085636138916,\n",
              "  5.940713882446289,\n",
              "  5.617124080657959,\n",
              "  5.778329372406006,\n",
              "  5.732450008392334,\n",
              "  5.804366588592529,\n",
              "  5.937497615814209,\n",
              "  5.801270008087158,\n",
              "  5.798190593719482,\n",
              "  6.236955642700195,\n",
              "  6.05514669418335,\n",
              "  6.1544647216796875,\n",
              "  7.492647647857666,\n",
              "  4.953392505645752,\n",
              "  12.19934368133545,\n",
              "  4.981857776641846,\n",
              "  4.907002925872803,\n",
              "  5.4898762702941895,\n",
              "  5.390432834625244,\n",
              "  5.652868747711182,\n",
              "  5.695486068725586,\n",
              "  5.684908390045166,\n",
              "  5.816646575927734,\n",
              "  5.657810688018799,\n",
              "  5.738499164581299,\n",
              "  6.0804314613342285,\n",
              "  6.260684967041016,\n",
              "  6.407377243041992,\n",
              "  6.3583221435546875,\n",
              "  6.564701080322266,\n",
              "  3.050706148147583,\n",
              "  7.116413593292236,\n",
              "  6.655613422393799,\n",
              "  4.586833477020264,\n",
              "  5.121962070465088,\n",
              "  5.748821258544922,\n",
              "  5.337578296661377,\n",
              "  5.547830104827881,\n",
              "  5.667308807373047,\n",
              "  5.671715259552002,\n",
              "  6.282565593719482,\n",
              "  6.038730621337891,\n",
              "  7.435659408569336,\n",
              "  7.642445087432861,\n",
              "  7.122045516967773,\n",
              "  7.036147594451904,\n",
              "  6.50335693359375,\n",
              "  7.070033550262451,\n",
              "  6.730352878570557,\n",
              "  6.837831497192383,\n",
              "  9.834712028503418,\n",
              "  6.27349853515625,\n",
              "  7.89442777633667,\n",
              "  8.71623420715332,\n",
              "  8.719769477844238,\n",
              "  8.642046928405762,\n",
              "  8.674779891967773,\n",
              "  8.638447761535645,\n",
              "  8.533391952514648,\n",
              "  9.40771770477295,\n",
              "  9.3689603805542,\n",
              "  8.779541969299316,\n",
              "  10.160308837890625,\n",
              "  8.716590881347656,\n",
              "  8.919960975646973,\n",
              "  9.01684856414795,\n",
              "  9.853483200073242,\n",
              "  9.623448371887207,\n",
              "  10.786799430847168,\n",
              "  14.789009094238281,\n",
              "  6.281993389129639,\n",
              "  9.494959831237793,\n",
              "  6.882473468780518,\n",
              "  6.097118377685547,\n",
              "  7.105350971221924,\n",
              "  9.341506004333496,\n",
              "  9.434884071350098,\n",
              "  9.487712860107422,\n",
              "  9.206786155700684,\n",
              "  9.11279296875,\n",
              "  9.345627784729004,\n",
              "  9.280523300170898,\n",
              "  9.148001670837402,\n",
              "  8.709189414978027,\n",
              "  8.787334442138672,\n",
              "  9.217474937438965,\n",
              "  9.274930000305176,\n",
              "  5.270236492156982,\n",
              "  6.729648113250732,\n",
              "  7.658936023712158,\n",
              "  8.786689758300781,\n",
              "  7.002012252807617,\n",
              "  7.937591075897217,\n",
              "  8.170809745788574,\n",
              "  8.453553199768066,\n",
              "  8.82674503326416,\n",
              "  9.168952941894531,\n",
              "  11.606468200683594,\n",
              "  7.670215129852295,\n",
              "  11.322251319885254,\n",
              "  11.01543140411377,\n",
              "  8.441417694091797,\n",
              "  8.214399337768555,\n",
              "  6.602554798126221,\n",
              "  9.21691608428955,\n",
              "  9.5184965133667,\n",
              "  9.299423217773438,\n",
              "  9.341689109802246,\n",
              "  9.91083812713623,\n",
              "  10.23766803741455,\n",
              "  11.12924575805664,\n",
              "  11.131142616271973,\n",
              "  11.167448997497559,\n",
              "  11.860886573791504,\n",
              "  12.227839469909668]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyf3CS9ZiHje"
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"Resnet_adam_plot.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlY5KnFmcSuB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}